# @package _group_
# Hydra configuration for chat.py

model_dir: artifacts/weights/Llama-3.2-3B 

# Optional: Path to the tokenizer model file (e.g., tokenizer.model or a directory for tiktoken).
# If not provided or null, assumes it's in model_dir.
tokenizer_path: null # or specify a path like "/path/to/tokenizer_files"

# Temperature for sampling.
# Higher values (e.g., 1.0) make output more random,
# lower (e.g., 0.1) make it more deterministic.
temperature: 0.7

# Maximum sequence length for the KVCache during chat.
# This determines the pre-allocated size of the cache.
# Should typically not exceed the model's own max_seq_len from its params.json.
max_seq_len: 1024 # Example value, adjust as needed

# The initial system prompt to guide the conversation.
system_prompt: "You are a helpful assistant." 