{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from models.llama.model import LLaMa\n",
    "from models.llama.config import ModelConfig\n",
    "from utils.kvcache import KVCache\n",
    "from models.llama.load import load_llama_weights\n",
    "from utils.memory import estimate_pytree_memory_footprint, format_bytes\n",
    "from models.llama.tokenizer import Tokenizer\n",
    "from sampling import Sampler\n",
    "from sampling import TopPSampler\n",
    "\n",
    "devices = jax.devices()\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=3072\n",
    "n_layers=28\n",
    "n_heads=24\n",
    "n_kv_heads=8\n",
    "ffn_hidden_dim=8192\n",
    "vocab_size=128256  \n",
    "max_seqlen = 8192\n",
    "rope_theta=500000.0\n",
    "rms_norm_eps=1e-5\n",
    "activation_fn=\"silu\"\n",
    "dtype=jnp.float32\n",
    "\n",
    "head_dim = dim // n_heads\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    ffn_hidden_dim=ffn_hidden_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seqlen=max_seqlen,\n",
    "    rope_theta=rope_theta,\n",
    "    rms_norm_eps=rms_norm_eps,\n",
    "    activation_fn=activation_fn,\n",
    "    dtype=dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unsupported ScalarType BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_llama_weights\n\u001b[32m      2\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m/home/ammar3.shaikh/ReLax/artifacts/weights/Llama-3.2-3B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m params = \u001b[43mload_llama_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ReLax/models/llama/load.py:42\u001b[39m, in \u001b[36mload_llama_weights\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m     39\u001b[39m params = {}\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Token embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m params[\u001b[33m'\u001b[39m\u001b[33mtok_embeddings\u001b[39m\u001b[33m'\u001b[39m] = {\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel.embed_tokens.weight\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Final normalization\u001b[39;00m\n\u001b[32m     45\u001b[39m params[\u001b[33m'\u001b[39m\u001b[33mnorm_weight\u001b[39m\u001b[33m'\u001b[39m] = jnp.asarray(tensors[\u001b[33m'\u001b[39m\u001b[33mmodel.norm.weight\u001b[39m\u001b[33m'\u001b[39m],dtype=config.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ReLax/.venv/lib/python3.11/site-packages/jax/_src/numpy/array.py:383\u001b[39m, in \u001b[36masarray\u001b[39m\u001b[34m(a, dtype, order, copy, device)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    382\u001b[39m   dtype = dtypes.canonicalize_dtype(dtype, allow_extended_dtype=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ReLax/.venv/lib/python3.11/site-packages/jax/_src/numpy/array.py:244\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(object, dtype, copy, order, ndmin, device)\u001b[39m\n\u001b[32m    238\u001b[39m out: ArrayLike\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(leaf, Array) \u001b[38;5;28;01mfor\u001b[39;00m leaf \u001b[38;5;129;01min\u001b[39;00m leaves):\n\u001b[32m    240\u001b[39m   \u001b[38;5;66;03m# TODO(jakevdp): falling back to numpy here fails to overflow for lists\u001b[39;00m\n\u001b[32m    241\u001b[39m   \u001b[38;5;66;03m# containing large integers; see discussion in\u001b[39;00m\n\u001b[32m    242\u001b[39m   \u001b[38;5;66;03m# https://github.com/jax-ml/jax/pull/6047. More correct would be to call\u001b[39;00m\n\u001b[32m    243\u001b[39m   \u001b[38;5;66;03m# coerce_to_array on each leaf, but this may have performance implications.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m   out = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, Array):\n\u001b[32m    246\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mobject\u001b[39m.aval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ReLax/.venv/lib/python3.11/site-packages/torch/_tensor.py:1227\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: Got unsupported ScalarType BFloat16"
     ]
    }
   ],
   "source": [
    "from models.llama.load import load_llama_weights\n",
    "model_path = \"/home/ammar3.shaikh/ReLax/artifacts/weights/Llama-3.2-3B\"\n",
    "params = load_llama_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\"/home/ammar3.shaikh/ReLax/artifacts/weights/Llama-3.2-3B/original/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLaMa(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(\n",
    "    model: LLaMa,\n",
    "    params,\n",
    "    tokenizer: Tokenizer,\n",
    "    prompt: str,\n",
    "    max_gen_len: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "    rng_key: jax.random.PRNGKey,\n",
    "):\n",
    "    \"\"\"\n",
    "    JAX-based text generation function.\n",
    "    \"\"\"\n",
    "    # 1. Initialize sampler and KVCache\n",
    "    sampler = TopPSampler(p=top_p, temperature=temperature)\n",
    "    kv_cache = KVCache.new(\n",
    "        n_layers=model.args.n_layers,\n",
    "        bsz=1,\n",
    "        max_seqlen=model.args.max_seqlen,\n",
    "        kv_heads=model.args.n_kv_heads,\n",
    "        head_dim=model.args.head_dim,\n",
    "        dtype=model.args.dtype,\n",
    "    )\n",
    "    print(f\"KVCache size: {format_bytes(estimate_pytree_memory_footprint(kv_cache))}\")\n",
    "\n",
    "    # 2. Define and JIT-compile the model step function for performance\n",
    "    @partial(jit, static_argnames=['model'])\n",
    "    def _model_step(model, params, tokens, kv_cache, start_pos):\n",
    "        logits, updated_kv_cache = model.apply(\n",
    "            {'params': params},\n",
    "            tokens,\n",
    "            start_pos=start_pos,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        return logits[:, -1, :], updated_kv_cache\n",
    "\n",
    "    # 3. Encode prompt and pre-fill KV cache\n",
    "    prompt_tokens = tokenizer.encode(prompt, bos=False, eos=False)\n",
    "    tokens = jnp.array([prompt_tokens], dtype=jnp.int32)\n",
    "    current_pos = 0\n",
    "    generated_tokens = list(prompt_tokens)\n",
    "\n",
    "    for _ in range(max_gen_len):\n",
    "\n",
    "        logits, kv_cache = _model_step(model, params, tokens, kv_cache, current_pos)\n",
    "        current_pos += tokens.shape[1]\n",
    "\n",
    "        rng_key, sample_key = random.split(rng_key)\n",
    "        next_token = sampler.sample(logits, sample_key)\n",
    "        generated_tokens.append(next_token.item())\n",
    "        tokens = next_token[:,None]\n",
    "\n",
    "    return tokenizer.decode(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KVCache size: 1.75GB\n"
     ]
    }
   ],
   "source": [
    "response = generate(model,params,tokenizer,prompt,32,0.6,0.9,jax.random.PRNGKey(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?  2 3 2 2 3 3 3 3 3 3 3 3 3 3 3 \n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
