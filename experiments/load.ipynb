{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from safetensors import safe_open\n",
    "from jax.sharding import PartitionSpec as PS,NamedSharding,Mesh\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "from models.llama.load import load_llama_weights\n",
    "from models.llama.model import LLaMA\n",
    "from utils.kvcache import KVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(\"/home/ammar3.shaikh/ReLax/artifacts/weights/llama-3.2-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mesh = jax.make_mesh((2, 2), ('a', 'b'))\n",
    "\n",
    "def mesh_sharding(\n",
    "    pspec: PS, mesh: Optional[Mesh] = None,\n",
    "  ) -> NamedSharding:\n",
    "  if mesh is None:\n",
    "    mesh = default_mesh\n",
    "  return NamedSharding(mesh, pspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_llama_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozen_dict_keys(['tok_embeddings', 'norm_weight', 'output', 'layers_0', 'layers_1', 'layers_2', 'layers_3', 'layers_4', 'layers_5', 'layers_6', 'layers_7', 'layers_8', 'layers_9', 'layers_10', 'layers_11', 'layers_12', 'layers_13', 'layers_14', 'layers_15', 'layers_16', 'layers_17', 'layers_18', 'layers_19', 'layers_20', 'layers_21', 'layers_22', 'layers_23', 'layers_24', 'layers_25', 'layers_26', 'layers_27'])\n"
     ]
    }
   ],
   "source": [
    "print(params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from models.llama.config import ModelConfig\n",
    "\n",
    "tmp_path = Path(\"/home/ammar3.shaikh/ReLax/experiments\")\n",
    "\n",
    "config_data = {\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_hidden_layers\": 2,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"num_key_value_heads\": 2,\n",
    "    \"intermediate_size\": 128,\n",
    "    \"vocab_size\": 1000,\n",
    "    \"rms_norm_eps\": 1e-6,\n",
    "    \"rope_theta\": 1000.0,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"hidden_act\": \"silu\"\n",
    "}\n",
    "\n",
    "config_path = tmp_path / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_data, f)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(vocab_size=1000, dim=64, ffn_hidden_dim=128, n_layers=2, n_heads=4, n_kv_heads=2, activation_fn='silu', max_seq_len=512, rope_theta=1000.0, rms_norm_eps=1e-06, mode='inference')\n"
     ]
    }
   ],
   "source": [
    "model_config = ModelConfig.from_json_file(\"/home/ammar3.shaikh/ReLax/experiments\")\n",
    "\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLaMA(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = jnp.array([[1,2,3,4,5,6,7,8,9,10]])\n",
    "kv_cache = KVCache.new(n_layers=model_config.n_layers,bsz=1,max_seq_len=model_config.max_seq_len,kv_heads=model_config.n_kv_heads,head_dim=model_config.head_dim)\n",
    "start_pos = 0\n",
    "params = model.init(jax.random.PRNGKey(0),tokens,start_pos,kv_cache)[\"params\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layer_0']['attention_norm_weight']\n",
      "['layer_0']['ffn_norm_weight']\n",
      "['layer_0']['w1_gate']\n",
      "['layer_0']['w2_up']\n",
      "['layer_0']['w3_down']\n",
      "['layer_0']['wk']\n",
      "['layer_0']['wo']\n",
      "['layer_0']['wq']\n",
      "['layer_0']['wv']\n",
      "['layer_1']['attention_norm_weight']\n",
      "['layer_1']['ffn_norm_weight']\n",
      "['layer_1']['w1_gate']\n",
      "['layer_1']['w2_up']\n",
      "['layer_1']['w3_down']\n",
      "['layer_1']['wk']\n",
      "['layer_1']['wo']\n",
      "['layer_1']['wq']\n",
      "['layer_1']['wv']\n",
      "['norm_weight']\n",
      "['output']['kernel']\n",
      "['tok_embeddings']['embedding']\n"
     ]
    }
   ],
   "source": [
    "def print_pytree_keys(pytree):\n",
    "  \"\"\"Prints the full path to each leaf in a PyTree.\"\"\"\n",
    "  jax.tree_util.tree_map_with_path(\n",
    "      # The lambda function is called for each leaf.\n",
    "      # `path` is a tuple of objects describing the route to the leaf.\n",
    "      # `_` is the leaf value itself, which we ignore here.\n",
    "      lambda path, _: print(jax.tree_util.keystr(path)),\n",
    "      pytree\n",
    "  )\n",
    "\n",
    "print_pytree_keys(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 4, 16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
