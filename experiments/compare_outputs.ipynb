{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from utils.ops import FeedForwardParams, feed_forward as feed_forward_jax\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"float32\")\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "from tests.torch_ops import FeedForward as FeedForward_torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = xm.xla_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables initialized\n",
      "jax output computed\n",
      "torch output computed\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Parameters\n",
    "bsz = 2\n",
    "seqlen = 64\n",
    "dim = 128\n",
    "multiple_of = 32\n",
    "dtype = np.float32\n",
    "\n",
    "# Mimic hidden_dim calculation from PyTorch implementation\n",
    "hidden_dim = int(2 * (4 * dim) / 3)\n",
    "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "# 2. Create shared weights and inputs\n",
    "np.random.seed(0)\n",
    "x_np = np.random.randn(bsz, seqlen, dim).astype(dtype)\n",
    "\n",
    "# PyTorch names weights w1, w2, w3. JAX uses w1_gate, w2_up, w3_down.\n",
    "# Mapping: torch.w1 -> jax.w1_gate, torch.w3 -> jax.w2_up, torch.w2 -> jax.w3_down\n",
    "w1_np = np.random.randn(dim, hidden_dim).astype(dtype) # gate_proj\n",
    "w3_np = np.random.randn(dim, hidden_dim).astype(dtype) # up_proj\n",
    "w2_np = np.random.randn(hidden_dim, dim).astype(dtype) # down_proj\n",
    "print(\"variables initialized\")\n",
    "\n",
    "# 3. JAX setup\n",
    "x_jax = jnp.array(x_np)\n",
    "jax_params = FeedForwardParams(\n",
    "w1_gate=jnp.array(w1_np),\n",
    "w2_up=jnp.array(w3_np),\n",
    "w3_down=jnp.array(w2_np)\n",
    ")\n",
    "output_jax = feed_forward_jax(x_jax, jax_params, activation_fn='silu')\n",
    "print(\"jax output computed\")\n",
    "\n",
    "# 4. PyTorch setup\n",
    "x_torch = torch.tensor(x_np, device=device)\n",
    "torch_ff = FeedForward_torch(dim=dim, hidden_dim=4 * dim, multiple_of=multiple_of, ffn_dim_multiplier=None)\n",
    "torch_ff.w1.weight = torch.nn.Parameter(torch.tensor(w1_np.T, device=device))\n",
    "torch_ff.w3.weight = torch.nn.Parameter(torch.tensor(w3_np.T, device=device))\n",
    "torch_ff.w2.weight = torch.nn.Parameter(torch.tensor(w2_np.T, device=device))\n",
    "output_torch = torch_ff(x_torch)\n",
    "print(\"torch output computed\")\n",
    "\n",
    "# 5. Compare\n",
    "#np.testing.assert_allclose(np.array(output_jax), output_torch.detach().cpu().numpy(), rtol=1e-4, atol=5e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1712.5023    1537.1199     219.94836  ...  1210.542     -517.8104\n",
      "    1957.5632  ]\n",
      "  [  720.4284     564.9929    1796.5664   ... -2025.019     1703.8607\n",
      "    1721.794   ]\n",
      "  [ -888.39954   -709.58875  -1181.3383   ... -2422.6638    -421.4978\n",
      "    1032.3291  ]\n",
      "  ...\n",
      "  [-2821.6235    -626.46783    510.15985  ...  -265.06628    560.21924\n",
      "    1006.6872  ]\n",
      "  [-2590.8398     302.2268   -1411.2493   ...   410.38287  -1112.2267\n",
      "   -1241.848   ]\n",
      "  [  372.734      959.9962   -2937.226    ... -3526.522     1063.862\n",
      "   -2356.3728  ]]\n",
      "\n",
      " [[-1457.0582    1140.365        9.848694 ...  3753.942    -1990.3994\n",
      "     710.10736 ]\n",
      "  [-1281.9932     480.12286    -14.375618 ...   834.6818    -536.7389\n",
      "   -1800.0834  ]\n",
      "  [  412.83234   1287.3164    1074.1871   ...   227.30594  -1567.2417\n",
      "    1311.1067  ]\n",
      "  ...\n",
      "  [ 1935.8376    3122.3125   -1176.6947   ... -2616.458      617.2828\n",
      "    1723.2715  ]\n",
      "  [  671.13477  -2396.4302    2321.6958   ...   687.7973    1850.6819\n",
      "   -1793.8928  ]\n",
      "  [-2118.3052   -1424.027    -3235.1694   ... -1625.9904     411.67975\n",
      "    1051.835   ]]]\n"
     ]
    }
   ],
   "source": [
    "print(output_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[[ 1727.29       1533.9014      210.12515   ...  1214.3501\n",
    "    -517.1265     1966.4995   ]\n",
    "  [  714.232       559.24133    1806.5352    ... -2020.8241\n",
    "    1712.354      1723.6455   ]\n",
    "  [ -883.953      -709.4646    -1173.58      ... -2415.8113\n",
    "    -417.66623    1023.7455   ]\n",
    "  ...\n",
    "  [-2821.728      -618.63855     513.56104   ...  -263.049\n",
    "     551.41956     995.66113  ]\n",
    "  [-2590.9634      300.09814   -1415.0653    ...   420.91208\n",
    "   -1106.2332    -1233.032    ]\n",
    "  [  365.25873     957.16455   -2939.5374    ... -3539.198\n",
    "    1062.811     -2341.3174   ]]\n",
    "\n",
    " [[-1453.0334     1150.6702        4.0286255 ...  3759.0645\n",
    "   -1990.8007      712.08453  ]\n",
    "  [-1276.685       479.71725     -21.86324   ...   830.7525\n",
    "    -537.3478    -1791.3425   ]\n",
    "  [  412.71432    1286.6497     1072.5814    ...   218.66956\n",
    "   -1575.5161     1316.7893   ]\n",
    "  ...\n",
    "  [ 1934.4199     3132.8506    -1170.0195    ... -2615.2393\n",
    "     618.4        1727.3142   ]\n",
    "  [  673.20856   -2402.7559     2321.2598    ...   677.0799\n",
    "    1856.0294    -1793.9155   ]\n",
    "  [-2118.0781    -1413.2001    -3229.768     ... -1631.8623\n",
    "     411.35004    1051.3274   ]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_torch.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "dev = xm.xla_device()\n",
    "\n",
    "x1 = torch.rand((3, 3)).to(dev)\n",
    "x2 = torch.rand((3, 8)).to(dev)\n",
    "\n",
    "y1 = torch.einsum('bs,st->bt', x1, x2)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([y1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
