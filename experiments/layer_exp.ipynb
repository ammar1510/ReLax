{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llama.config import ModelConfig\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "from models.llama.model import TransformerBlock as TransformerBlockJax\n",
    "from models.llama.config import ModelConfig\n",
    "from experiments.torch_llama import TransformerBlock as TransformerBlockTorch, ModelArgs\n",
    "from utils.ops import precompute_freqs_cis as precompute_freqs_cis_jax\n",
    "from utils.kvcache import KVCache\n",
    "from experiments.torch_llama import precompute_freqs_cis as precompute_freqs_cis_torch\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 768\n",
    "n_layers = 8\n",
    "n_heads = 12\n",
    "n_kv_heads = 4\n",
    "ffn_hidden_dim = 2048\n",
    "head_dim = 64\n",
    "vocab_size = 1000\n",
    "multiple_of = 256   \n",
    "norm_eps = 1e-5\n",
    "rope_theta = 500000\n",
    "use_scaled_rope = False\n",
    "max_batch_size = 4\n",
    "max_seq_len = 1024\n",
    "flash = False\n",
    "\n",
    "args = ModelArgs(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    ffn_hidden_dim=ffn_hidden_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    multiple_of=multiple_of,\n",
    "    norm_eps=norm_eps,\n",
    "    rope_theta=rope_theta,\n",
    "    use_scaled_rope=False,\n",
    "    max_batch_size=4,\n",
    "    max_seq_len=max_seq_len,\n",
    "    flash=flash,\n",
    ")\n",
    "\n",
    "config = ModelConfig(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    ffn_hidden_dim=ffn_hidden_dim,\n",
    "    rms_norm_eps=norm_eps,\n",
    "    activation_fn=\"silu\",\n",
    "    dtype=\"float32\",\n",
    "    max_seqlen=max_seq_len, \n",
    "    vocab_size=vocab_size,\n",
    "    rope_theta=rope_theta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block_jax = TransformerBlockJax(config)\n",
    "transformer_block_torch = TransformerBlockTorch(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 32, 2)\n",
      "torch.Size([2048, 32, 2])\n"
     ]
    }
   ],
   "source": [
    "freqs_cis_jax = precompute_freqs_cis_jax(dim//n_heads, end=2*max_seq_len, theta=rope_theta)\n",
    "freqs_cis_torch = precompute_freqs_cis_torch(dim//n_heads, end=2*max_seq_len, theta=rope_theta)\n",
    "\n",
    "print(freqs_cis_jax.shape)\n",
    "print(freqs_cis_torch.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "assert np.allclose(np.array(freqs_cis_jax), np.array(freqs_cis_torch.cpu().numpy()), atol=2e-4,rtol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_shared_transformer_weights(dim, n_heads, n_kv_heads, ffn_hidden_dim, seed=42):\n",
    "    \"\"\"Create shared weights for JAX and PyTorch TransformerBlocks\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    head_dim = dim // n_heads\n",
    "    \n",
    "    # Create numpy weights\n",
    "    weights_np = {\n",
    "        # Attention weights\n",
    "        'wq': np.random.normal(0, 0.02, (dim, n_heads * head_dim)).astype(np.float32),\n",
    "        'wk': np.random.normal(0, 0.02, (dim, n_kv_heads * head_dim)).astype(np.float32),\n",
    "        'wv': np.random.normal(0, 0.02, (dim, n_kv_heads * head_dim)).astype(np.float32),\n",
    "        'wo': np.random.normal(0, 0.02, (n_heads * head_dim, dim)).astype(np.float32),\n",
    "        \n",
    "        # Feed-forward weights  \n",
    "        'w1': np.random.normal(0, 0.02, (dim, ffn_hidden_dim)).astype(np.float32),\n",
    "        'w2': np.random.normal(0, 0.02, (ffn_hidden_dim, dim)).astype(np.float32),\n",
    "        'w3': np.random.normal(0, 0.02, (dim, ffn_hidden_dim)).astype(np.float32),\n",
    "        \n",
    "        # Normalization weights\n",
    "        'attention_norm': np.ones(dim, dtype=np.float32),\n",
    "        'ffn_norm': np.ones(dim, dtype=np.float32),\n",
    "    }\n",
    "    \n",
    "    # Convert to PyTorch format\n",
    "    weights_torch = {k: torch.from_numpy(v) for k, v in weights_np.items()}\n",
    "    \n",
    "    # Convert to JAX format (reshape attention weights for JAX)\n",
    "    weights_jax = {\n",
    "        'wq': jnp.array(weights_np['wq'].reshape(dim, n_heads, head_dim)),\n",
    "        'wk': jnp.array(weights_np['wk'].reshape(dim, n_kv_heads, head_dim)),\n",
    "        'wv': jnp.array(weights_np['wv'].reshape(dim, n_kv_heads, head_dim)),\n",
    "        'wo': jnp.array(weights_np['wo']),\n",
    "        'w1_gate': jnp.array(weights_np['w1']),\n",
    "        'w2_up': jnp.array(weights_np['w3']),\n",
    "        'w3_down': jnp.array(weights_np['w2']),\n",
    "        'attention_norm_weight': jnp.array(weights_np['attention_norm']),\n",
    "        'ffn_norm_weight': jnp.array(weights_np['ffn_norm']),\n",
    "    }\n",
    "    \n",
    "    return weights_torch, weights_jax\n",
    "\n",
    "def set_torch_weights(transformer_block_torch, weights_torch):\n",
    "    \"\"\"Set weights for PyTorch TransformerBlock\"\"\"\n",
    "    with torch.no_grad():\n",
    "        transformer_block_torch.attention.wq.weight.copy_(weights_torch['wq'].T)\n",
    "        transformer_block_torch.attention.wk.weight.copy_(weights_torch['wk'].T)\n",
    "        transformer_block_torch.attention.wv.weight.copy_(weights_torch['wv'].T)\n",
    "        transformer_block_torch.attention.wo.weight.copy_(weights_torch['wo'].T)\n",
    "        \n",
    "        transformer_block_torch.feed_forward.w1.weight.copy_(weights_torch['w1'].T)\n",
    "        transformer_block_torch.feed_forward.w2.weight.copy_(weights_torch['w2'].T)\n",
    "        transformer_block_torch.feed_forward.w3.weight.copy_(weights_torch['w3'].T)\n",
    "        \n",
    "        transformer_block_torch.attention_norm.weight.copy_(weights_torch['attention_norm'])\n",
    "        transformer_block_torch.ffn_norm.weight.copy_(weights_torch['ffn_norm'])\n",
    "\n",
    "def set_jax_weights(jax_variables, weights_jax):\n",
    "    \"\"\"Set weights for JAX TransformerBlock variables\"\"\"\n",
    "    new_params = jax_variables['params'].copy()\n",
    "    \n",
    "    new_params['wq'] = weights_jax['wq']\n",
    "    new_params['wk'] = weights_jax['wk']\n",
    "    new_params['wv'] = weights_jax['wv']\n",
    "    new_params['wo'] = weights_jax['wo']\n",
    "    \n",
    "    new_params['w1_gate'] = weights_jax['w1_gate']\n",
    "    new_params['w2_up'] = weights_jax['w2_up']\n",
    "    new_params['w3_down'] = weights_jax['w3_down']\n",
    "    \n",
    "    new_params['attention_norm_weight'] = weights_jax['attention_norm_weight']\n",
    "    new_params['ffn_norm_weight'] = weights_jax['ffn_norm_weight']\n",
    "    \n",
    "    return {'params': new_params}\n",
    "\n",
    "# Usage:\n",
    "weights_torch, weights_jax = create_shared_transformer_weights(dim, n_heads, n_kv_heads, ffn_hidden_dim)\n",
    "set_torch_weights(transformer_block_torch, weights_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "np_tokens = np.random.uniform(0, 10, (max_batch_size, seq_len, dim)).astype(np.float32)  # Shape: (2, 512, 768)\n",
    "\n",
    "torch_mask = torch.full((seq_len, seq_len), float(\"-inf\"))\n",
    "torch_mask = torch.triu(torch_mask, diagonal=1)\n",
    "\n",
    "torch_output = transformer_block_torch(torch.tensor(np_tokens).float(), start_pos=0, freqs_cis=freqs_cis_torch[:seq_len], mask=torch_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "dummy_input = jnp.ones((max_batch_size, 1, dim),dtype=jnp.float32)  # dummy input for initialization\n",
    "dummy_freqs = freqs_cis_jax[:1]\n",
    "dummy_kvcache = KVCache.new(n_layers=config.n_layers, bsz=max_batch_size, max_seq_len=config.max_seqlen, kv_heads=config.n_kv_heads, head_dim=config.head_dim,dtype=jnp.float32)\n",
    "variables = transformer_block_jax.init(key, dummy_input, dummy_freqs, dummy_kvcache, 0, 0)\n",
    "variables = set_jax_weights(variables, weights_jax)\n",
    "\n",
    "\n",
    "kvcache = KVCache.new(n_layers=config.n_layers, bsz=max_batch_size, max_seq_len=config.max_seqlen, kv_heads=config.n_kv_heads, head_dim=config.head_dim,dtype=jnp.float32)\n",
    "\n",
    "# Now apply it with the actual inputs\n",
    "jax_output, _ = transformer_block_jax.apply(variables, np_tokens, freqs_cis_jax, kvcache, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
