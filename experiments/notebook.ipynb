{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44157b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2f9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51e660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # hidden dim gymnastics that Meta simplified only later\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.w1(x)\n",
    "        up = self.w3(x)\n",
    "        return self.w2(F.silu(gate) * up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607913e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@struct.dataclass\n",
    "class FeedForwardParams:\n",
    "    w_gate: jax.Array  # Corresponds to gate_proj\n",
    "    w_up: jax.Array  # Corresponds to up_proj\n",
    "    w_down: jax.Array  # Corresponds to down_proj\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"activation_fn\"],donate_argnums=[0])\n",
    "def feed_forward_jax(\n",
    "    x: jax.Array,\n",
    "    params: FeedForwardParams,\n",
    "    activation_fn: str,  # Added activation function name\n",
    ") -> jax.Array:\n",
    "    \"\"\"\n",
    "    Compute FeedForward network (MLP) using a configurable activation function (like SwiGLU).\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape [batch_size, seqlen, dim].\n",
    "        params: Dataclass containing weight matrices (w_gate, w_up, w_down).\n",
    "        activation_fn: Name of the activation function ('silu', 'relu', 'gelu').\n",
    "\n",
    "    Returns:\n",
    "        Output tensor after MLP computation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Project input: x -> gate, up\n",
    "    # x: [bs, seqlen, dim], w_gate: [dim, hidden_dim], w_up: [dim, hidden_dim]\n",
    "    gate = jnp.einsum(\"bsd,dh->bsh\", x, params.w_gate)\n",
    "    up = jnp.einsum(\"bsd,dh->bsh\", x, params.w_up)\n",
    "\n",
    "    # Apply the specified activation function (SwiGLU style)\n",
    "    if activation_fn == \"silu\":\n",
    "        activated_gate = jax.nn.silu(gate)\n",
    "    elif activation_fn == \"relu\":\n",
    "        activated_gate = jax.nn.relu(gate)\n",
    "    elif activation_fn == \"gelu\":\n",
    "        # Use approximate=False for exact GELU, True for faster approximation\n",
    "        activated_gate = jax.nn.gelu(gate, approximate=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
    "        # replace error handling with chex\n",
    "\n",
    "    fused_activation = activated_gate * up\n",
    "\n",
    "    # Project down\n",
    "    # fused_swiglu: [bs, seqlen, hidden_dim], w_down: [hidden_dim, dim]\n",
    "    output = jnp.einsum(\"bsh,hd->bsd\", fused_activation, params.w_down)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd78db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 2\n",
    "seqlen = 64\n",
    "dim = 192\n",
    "multiple_of = 32\n",
    "dtype = np.float32\n",
    "\n",
    "# Mimic hidden_dim calculation from PyTorch implementation\n",
    "hidden_dim = int(2 * (4 * dim) / 3)\n",
    "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "# 2. Create shared weights and inputs\n",
    "np.random.seed(0)\n",
    "x_np = np.random.randn(bsz, seqlen, dim).astype(dtype)\n",
    "\n",
    "# PyTorch names weights w1, w2, w3. JAX uses w_gate, w_up, w_down.\n",
    "# Mapping: torch.w1 -> jax.w_gate, torch.w3 -> jax.w_up, torch.w2 -> jax.w_down\n",
    "w1_np = np.random.randn(dim, hidden_dim).astype(dtype)  # gate_proj\n",
    "w3_np = np.random.randn(dim, hidden_dim).astype(dtype)  # up_proj\n",
    "w2_np = np.random.randn(hidden_dim, dim).astype(dtype)  # down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2240e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. JAX setup\n",
    "x_jax = jnp.array(x_np)\n",
    "jax_params = FeedForwardParams(\n",
    "    w_gate=jnp.array(w1_np), w_up=jnp.array(w3_np), w_down=jnp.array(w2_np)\n",
    ")\n",
    "output_jax = feed_forward_jax(x_jax, jax_params,\"silu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86795cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate[0,:5,0]\n",
      " tensor([  2.8536,  -9.0668,  28.0873, -16.5318,  -2.2464],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "up[0,:5,0]\n",
      " tensor([ 16.0499, -10.1857,  -5.5634, -21.5287,  -1.0652],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "w1.weight[0,:5]\n",
      " tensor([-0.2965, -0.1555,  2.1040, -1.6559, -0.6499], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"PJRT_DEVICE\"]=\"TPU\"\n",
    "# 4. PyTorch setup\n",
    "x_torch = torch.tensor(x_np, device=device)\n",
    "torch_ff = FeedForward(\n",
    "    dim=dim, hidden_dim=hidden_dim, multiple_of=multiple_of, ffn_dim_multiplier=None\n",
    ")\n",
    "torch_ff.w1.weight = torch.nn.Parameter(torch.tensor(w1_np.T, device=device))\n",
    "torch_ff.w3.weight = torch.nn.Parameter(torch.tensor(w3_np.T, device=device))\n",
    "torch_ff.w2.weight = torch.nn.Parameter(torch.tensor(w2_np.T, device=device))\n",
    "output_torch = torch_ff(x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3824.567   3250.896  -1562.5957  4669.1475 -2415.9502]\n",
      "tensor([ 3824.5669,  3250.8965, -1562.5955,  4669.1475, -2415.9492],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_jax[0,:5,0])\n",
    "print(output_torch[0,:5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14c82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd8471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(1, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@jax.jit\n",
    "def f(x,y):\n",
    "    print(type(x))\n",
    "    print(type(y))\n",
    "    return x+y\n",
    "\n",
    "f(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf52ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
