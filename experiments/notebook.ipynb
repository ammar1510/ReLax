{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaf52ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Must be done before importing JAX or any JAX-related modules\n",
    "os.environ[\"TPU_PROCESS_BOUNDS\"] = \"1,1,1\"\n",
    "os.environ[\"TPU_VISIBLE_CHIPS\"] = \"0\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "import dataclasses\n",
    "from models.llama.model import LLaMa \n",
    "from models.llama.config import ModelConfig\n",
    "from models.llama.load import load_llama_weights\n",
    "from models.llama.tokenizer import Tokenizer\n",
    "from utils.kvcache import KVCache\n",
    "\n",
    "print(jax.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4dc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/home/ammar/weights/Llama-3.2-1B\")\n",
    "jax_dtype = jnp.bfloat16\n",
    "use_scaled_rope = False\n",
    "batch_size = 1\n",
    "max_seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29f9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig.from_json_file(model_path)\n",
    "config = dataclasses.replace(config, dtype=jax_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6762a447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(vocab_size=128256, dim=2048, ffn_hidden_dim=8192, n_layers=16, n_heads=32, n_kv_heads=8, activation_fn='silu', max_seqlen=8192, rope_theta=500000.0, rms_norm_eps=1e-05, mode='inference', dtype=<class 'jax.numpy.bfloat16'>, use_scaled_rope=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45081699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ JAX model initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize JAX model\n",
    "model = LLaMa(config)\n",
    "print(\"✓ JAX model initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dbdcbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/ammar/weights/Llama-3.2-1B\n",
      "Found 1 safetensor file(s)\n",
      "  Loading model.safetensors...\n",
      "Loaded 146 tensors total\n",
      "Converting HuggingFace weights to ReLax format...\n",
      "  ✓ Embeddings: (128256, 2048)\n",
      "  ✓ LM head (tied embeddings): (128256, 2048) -> (2048, 128256)\n",
      "  ✓ Final norm: (2048,)\n",
      "  Converting 16 transformer layers...\n",
      "    Layer 0 shapes:\n",
      "      wq: (2048, 32, 64)\n",
      "      wk: (2048, 8, 64)\n",
      "      wv: (2048, 8, 64)\n",
      "      wo: (2048, 2048)\n",
      "      w_gate: (2048, 8192)\n",
      "      w_up: (2048, 8192)\n",
      "      w_down: (8192, 2048)\n",
      "      attention_norm_weight: (2048,)\n",
      "      ffn_norm_weight: (2048,)\n",
      "  ✓ Converted all 16 layers\n"
     ]
    }
   ],
   "source": [
    "loaded_params = load_llama_weights(str(model_path), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a576e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\n\\nToday Date: 23 July 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def526c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "Prompt tokens: 111\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = model_path / \"original\" / \"tokenizer.model\"\n",
    "if not tokenizer_path.exists():\n",
    "    tokenizer_path = model_path / \"tokenizer.model\"\n",
    "\n",
    "if not tokenizer_path.exists():\n",
    "    raise FileNotFoundError(f\"Tokenizer not found at {tokenizer_path}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(model_path=str(tokenizer_path))\n",
    "\n",
    "# Tokenize input text\n",
    "prompt_tokens = tokenizer.encode(test_prompt, bos=False, eos=False)\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "print(f\"Prompt tokens: {len(prompt_tokens)}\")\n",
    "\n",
    "max_gen_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e61c65d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe402b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING TEXT WITH GREEDY SAMPLING\n",
      "================================================================================\n",
      "✓ KV cache initialized: 16 layers, batch=1, max_seq_len=367\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING TEXT WITH GREEDY SAMPLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize KV cache for generation\n",
    "max_seq_len = len(prompt_tokens) + max_gen_len\n",
    "head_dim = config.head_dim\n",
    "kv_cache = KVCache.new(\n",
    "    config.n_layers,\n",
    "    batch_size,\n",
    "    max_seq_len,\n",
    "    config.n_kv_heads,\n",
    "    head_dim,\n",
    "    dtype=jax_dtype\n",
    ")\n",
    "print(f\"✓ KV cache initialized: {config.n_layers} layers, \"\n",
    "        f\"batch={batch_size}, max_seq_len={max_seq_len}\")\n",
    "\n",
    "# Convert prompt tokens to JAX array\n",
    "tokens = jnp.array([prompt_tokens], dtype=jnp.int32)  # [1, prompt_len]\n",
    "current_seq_len = len(prompt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec198b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling with 111 tokens...\n",
      "✓ Prefill complete\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"Prefilling with {len(prompt_tokens)} tokens...\")\n",
    "seq_lengths = jnp.array([len(prompt_tokens)], dtype=jnp.int32)\n",
    "logits, kv_cache = model.apply(\n",
    "    {\"params\": loaded_params},\n",
    "    tokens=tokens,\n",
    "    seq_lengths=seq_lengths,\n",
    "    kv_cache=kv_cache,\n",
    ")\n",
    "print(f\"✓ Prefill complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "003296b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 112, 128256)\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cb583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tokens = jnp.ones((batch_size,1),dtype=jnp.int32)\n",
    "seq_lengths = jnp.ones((batch_size,),dtype=jnp.int32)\n",
    "kv_cache = KVCache.new(\n",
    "    config.n_layers,\n",
    "    batch_size,\n",
    "    max_seq_len,\n",
    "    config.n_kv_heads,\n",
    "    config.head_dim,\n",
    "    dtype=config.dtype\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97021e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(model_key,dummy_tokens,seq_lengths,kv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2cb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ammar/ReLax/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1280: UserWarning: Some donated buffers were not usable: bfloat16[16384,32,2].\n",
      "See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
      "  warnings.warn(\"Some donated buffers were not usable:\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae02e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['norm_weight', 'output', 'tok_embeddings', 'layer_0', 'layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15'])\n"
     ]
    }
   ],
   "source": [
    "print(params[\"params\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851a3978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef({'layer_0': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_1': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_10': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_11': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_12': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_13': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_14': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_15': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_2': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_3': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_4': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_5': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_6': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_7': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_8': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_9': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'norm_weight': *, 'output': *, 'tok_embeddings': {'embedding': *}})\n",
      "PyTreeDef({'layer_0': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_1': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_10': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_11': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_12': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_13': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_14': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_15': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_2': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_3': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_4': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_5': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_6': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_7': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_8': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'layer_9': {'attention_norm_weight': *, 'ffn_norm_weight': *, 'w_down': *, 'w_gate': *, 'w_up': *, 'wk': *, 'wo': *, 'wq': *, 'wv': *}, 'norm_weight': *, 'output': *, 'tok_embeddings': {'embedding': *}})\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "print(jax.tree.structure(params[\"params\"]))\n",
    "print(jax.tree.structure(loaded_params))\n",
    "if jax.tree.structure(params[\"params\"]) != jax.tree.structure(loaded_params):\n",
    "    raise ValueError(\"params and loaded_params do not share the same pytree structure\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d82cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_1': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_10': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_11': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_12': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_13': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_14': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_15': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_2': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_3': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_4': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_5': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_6': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_7': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_8': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'layer_9': {'attention_norm_weight': True,\n",
       "  'ffn_norm_weight': True,\n",
       "  'w_down': True,\n",
       "  'w_gate': True,\n",
       "  'w_up': True,\n",
       "  'wk': True,\n",
       "  'wo': True,\n",
       "  'wq': True,\n",
       "  'wv': True},\n",
       " 'norm_weight': True,\n",
       " 'output': True,\n",
       " 'tok_embeddings': {'embedding': True}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree.map(lambda x,y: x.shape == y.shape, params[\"params\"], loaded_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae095e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309836c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PYTORCH MODEL SETUP\n",
    "# ============================================================================\n",
    "import torch\n",
    "\n",
    "# Device selection for PyTorch\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    TORCH_XLA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_XLA_AVAILABLE = False\n",
    "    torch_xla = None\n",
    "\n",
    "if TORCH_XLA_AVAILABLE:\n",
    "    torch_device = torch_xla.device()\n",
    "    print(f\"Using PyTorch XLA device: {torch_device}\")\n",
    "elif torch.cuda.is_available():\n",
    "    torch_device = \"cuda\"\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    torch_device = \"cpu\"\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# Import PyTorch model components\n",
    "from experiments.torch_llama import Llama as Llama_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PyTorch model\n",
    "torch_original_path = model_path / \"original\"\n",
    "torch_tokenizer_path = torch_original_path / \"tokenizer.model\"\n",
    "\n",
    "if not torch_original_path.exists():\n",
    "    torch_original_path = model_path\n",
    "    torch_tokenizer_path = model_path / \"tokenizer.model\"\n",
    "\n",
    "print(f\"Loading PyTorch model from: {torch_original_path}\")\n",
    "print(f\"Tokenizer: {torch_tokenizer_path}\")\n",
    "\n",
    "# Calculate max_seq_len needed\n",
    "max_seq_len = len(prompt_tokens) + max_gen_len\n",
    "max_seq_len = min(max_seq_len, 8192)  # Cap at model max\n",
    "\n",
    "llama_wrapper = Llama_wrapper.build(\n",
    "    ckpt_dir=str(torch_original_path),\n",
    "    tokenizer_path=str(torch_tokenizer_path),\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=1,\n",
    "    flash=False,\n",
    ")\n",
    "torch_model = llama_wrapper.model\n",
    "torch_model.eval()\n",
    "print(\"✓ PyTorch model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00779b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PYTORCH FORWARD PASS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PYTORCH FORWARD PASS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get device for tensor creation\n",
    "if TORCH_XLA_AVAILABLE:\n",
    "    device_for_tensor = torch_device\n",
    "elif isinstance(torch_device, str):\n",
    "    device_for_tensor = torch_device\n",
    "else:\n",
    "    device_for_tensor = torch_device\n",
    "\n",
    "# Initialize KV caches in all layers\n",
    "torch_params = torch_model.params\n",
    "total_len = len(prompt_tokens) + max_gen_len\n",
    "total_len = min(total_len, max_seq_len)\n",
    "\n",
    "print(f\"Initializing KV caches for max sequence length: {total_len}\")\n",
    "from experiments.torch_llama import KVCache as KVCache_torch\n",
    "for i in range(torch_params.n_layers):\n",
    "    layer_dtype = torch_model.layers[i].attention.wq.weight.dtype\n",
    "    layer_device = torch_model.layers[i].attention.wq.weight.device\n",
    "    torch_model.layers[i].attention.cache = KVCache_torch(\n",
    "        batch_size=1,\n",
    "        seq_length=total_len,\n",
    "        n_kv_heads=torch_params.n_kv_heads,\n",
    "        head_dim=torch_params.dim // torch_params.n_heads,\n",
    "        dtype=layer_dtype,\n",
    "        device=layer_device,\n",
    "    )\n",
    "print(f\"✓ KV caches initialized in all {torch_params.n_layers} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prompt tokens to tensor\n",
    "tokens_torch = torch.tensor([prompt_tokens], device=device_for_tensor, dtype=torch.long)  # [1, prompt_len]\n",
    "start_pos = 0\n",
    "\n",
    "# Prefill: process the prompt\n",
    "print(f\"Prefilling with {len(prompt_tokens)} tokens...\")\n",
    "with torch.no_grad():\n",
    "    logits_torch = torch_model.forward_inference(tokens_torch, start_pos)\n",
    "    # Synchronize XLA operations if using XLA\n",
    "    if TORCH_XLA_AVAILABLE:\n",
    "        torch_xla.sync()  # Synchronize XLA operations\n",
    "print(f\"✓ Prefill complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logits shape\n",
    "print(f\"PyTorch logits shape: {logits_torch.shape}\")\n",
    "print(f\"JAX logits shape: {logits.shape}\")\n",
    "print(f\"Shapes match: {logits_torch.shape == logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453e269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
