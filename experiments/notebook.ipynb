{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44157b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2f9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51e660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # hidden dim gymnastics that Meta simplified only later\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.w1(x)\n",
    "        up = self.w3(x)\n",
    "        return self.w2(F.silu(gate) * up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 2\n",
    "seqlen = 64\n",
    "dim = 192\n",
    "multiple_of = 32\n",
    "dtype = np.float32\n",
    "\n",
    "# Mimic hidden_dim calculation from PyTorch implementation\n",
    "hidden_dim = int(2 * (4 * dim) / 3)\n",
    "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "# 2. Create shared weights and inputs\n",
    "np.random.seed(0)\n",
    "x_np = np.random.randn(bsz, seqlen, dim).astype(dtype)\n",
    "\n",
    "# PyTorch names weights w1, w2, w3. JAX uses w_gate, w_up, w_down.\n",
    "# Mapping: torch.w1 -> jax.w_gate, torch.w3 -> jax.w_up, torch.w2 -> jax.w_down\n",
    "w1_np = np.random.normal(0, 0.02, (dim, hidden_dim)).astype(dtype)  # gate_proj\n",
    "w3_np = np.random.normal(0, 0.02, (dim, hidden_dim)).astype(dtype)  # up_proj\n",
    "w2_np = np.random.normal(0, 0.02, (hidden_dim, dim)).astype(dtype)  # down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4669131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Initialize attention weights like Flax nn.initializers.normal(stddev=0.02)\n",
    "# Define attention parameters (like in your LLaMa model)\n",
    "n_heads = 8\n",
    "n_kv_heads = 4  # For grouped query attention\n",
    "head_dim = dim // n_heads\n",
    "\n",
    "# Method 1: Using np.random.normal directly (equivalent to nn.initializers.normal(stddev=0.02))\n",
    "wq_np = np.random.normal(0, 0.02, (dim, n_heads, head_dim)).astype(dtype)\n",
    "wk_np = np.random.normal(0, 0.02, (dim, n_kv_heads, head_dim)).astype(dtype)\n",
    "wv_np = np.random.normal(0, 0.02, (dim, n_kv_heads, head_dim)).astype(dtype)\n",
    "wo_np = np.random.normal(0, 0.02, (n_heads * head_dim, dim)).astype(dtype)\n",
    "\n",
    "# Method 2: Using np.random.randn and scaling (equivalent approach)\n",
    "# wv_np = (np.random.randn(dim, n_kv_heads, head_dim) * 0.02).astype(dtype)\n",
    "\n",
    "print(f\"wv shape: {wv_np.shape}\")\n",
    "print(f\"wv std: {wv_np.std():.6f} (should be ~0.02)\")\n",
    "print(f\"wv mean: {wv_np.mean():.6f} (should be ~0.0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86795cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PyTorch setup\n",
    "x_torch = torch.tensor(x_np, device=device)\n",
    "torch_ff = FeedForward(\n",
    "    dim=dim, hidden_dim=hidden_dim, multiple_of=multiple_of, ffn_dim_multiplier=None\n",
    ")\n",
    "torch_ff.w1.weight = torch.nn.Parameter(torch.tensor(w1_np.T, device=device))\n",
    "torch_ff.w3.weight = torch.nn.Parameter(torch.tensor(w3_np.T, device=device))\n",
    "torch_ff.w2.weight = torch.nn.Parameter(torch.tensor(w2_np.T, device=device))\n",
    "output_torch = torch_ff(x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81775f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3824.5667,  3250.8960, -1562.5957,  4669.1479, -2415.9500],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch[0,:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a4f0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@struct.dataclass\n",
    "class FeedForwardParams:\n",
    "    w_gate: jax.Array  # Corresponds to gate_proj\n",
    "    w_up: jax.Array  # Corresponds to up_proj\n",
    "    w_down: jax.Array  # Corresponds to down_proj\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"activation_fn\"],donate_argnums=[0])\n",
    "def feed_forward_jax(\n",
    "    x: jax.Array,\n",
    "    params: FeedForwardParams,\n",
    "    activation_fn: str,  # Added activation function name\n",
    ") -> jax.Array:\n",
    "    \"\"\"\n",
    "    Compute FeedForward network (MLP) using a configurable activation function (like SwiGLU).\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape [batch_size, seqlen, dim].\n",
    "        params: Dataclass containing weight matrices (w_gate, w_up, w_down).\n",
    "        activation_fn: Name of the activation function ('silu', 'relu', 'gelu').\n",
    "\n",
    "    Returns:\n",
    "        Output tensor after MLP computation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Project input: x -> gate, up\n",
    "    # x: [bs, seqlen, dim], w_gate: [dim, hidden_dim], w_up: [dim, hidden_dim]\n",
    "    gate = jnp.einsum(\"bsd,dh->bsh\", x, params.w_gate)\n",
    "    up = jnp.einsum(\"bsd,dh->bsh\", x, params.w_up)\n",
    "\n",
    "    # Apply the specified activation function (SwiGLU style)\n",
    "    if activation_fn == \"silu\":\n",
    "        activated_gate = jax.nn.silu(gate)\n",
    "    elif activation_fn == \"relu\":\n",
    "        activated_gate = jax.nn.relu(gate)\n",
    "    elif activation_fn == \"gelu\":\n",
    "        # Use approximate=False for exact GELU, True for faster approximation\n",
    "        activated_gate = jax.nn.gelu(gate, approximate=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
    "        # replace error handling with chex\n",
    "\n",
    "    fused_activation = activated_gate * up\n",
    "\n",
    "    # Project down\n",
    "    # fused_swiglu: [bs, seqlen, hidden_dim], w_down: [hidden_dim, dim]\n",
    "    output = jnp.einsum(\"bsh,hd->bsd\", fused_activation, params.w_down)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247ca6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. JAX setup\n",
    "x_jax = jnp.array(x_np)\n",
    "jax_params = FeedForwardParams(\n",
    "    w_gate=jnp.array(w1_np), w_up=jnp.array(w3_np), w_down=jnp.array(w2_np)\n",
    ")\n",
    "output_jax = feed_forward_jax(x_jax, jax_params,\"silu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59aebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3821.081   3259.9338 -1562.5793  4675.722  -2414.2063]\n",
      "tensor([ 3824.5667,  3250.8960, -1562.5957,  4669.1479, -2415.9500],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_jax[0,:5,0])\n",
    "print(output_torch[0,:5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c82d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
