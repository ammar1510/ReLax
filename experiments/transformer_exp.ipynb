{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from models.llama.config import ModelConfig\n",
    "from models.llama.model import LLaMa as TransformerJax\n",
    "from models.llama.config import ModelConfig\n",
    "from experiments.torch_llama import Transformer as TransformerTorch, ModelArgs\n",
    "from utils.ops import precompute_freqs_cis as precompute_freqs_cis_jax\n",
    "from utils.kvcache import KVCache\n",
    "from experiments.torch_llama import precompute_freqs_cis as precompute_freqs_cis_torch\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"float32\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 768\n",
    "n_layers = 8\n",
    "n_heads = 12\n",
    "n_kv_heads = 4\n",
    "ffn_hidden_dim = 2048\n",
    "head_dim = 64\n",
    "vocab_size = 1000\n",
    "multiple_of = 256   \n",
    "norm_eps = 1e-5\n",
    "rope_theta = 500000\n",
    "use_scaled_rope = False\n",
    "max_batch_size = 8\n",
    "max_seq_len = 1024\n",
    "flash = False\n",
    "\n",
    "args = ModelArgs(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    ffn_hidden_dim=ffn_hidden_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    multiple_of=multiple_of,\n",
    "    norm_eps=norm_eps,\n",
    "    rope_theta=rope_theta,\n",
    "    use_scaled_rope=False,\n",
    "    max_batch_size=4,\n",
    "    max_seq_len=max_seq_len,\n",
    "    flash=flash,\n",
    ")\n",
    "\n",
    "config = ModelConfig(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    ffn_hidden_dim=ffn_hidden_dim,\n",
    "    rms_norm_eps=norm_eps,\n",
    "    activation_fn=\"silu\",\n",
    "    dtype=\"float32\",\n",
    "    max_seqlen=max_seq_len, \n",
    "    vocab_size=vocab_size,\n",
    "    rope_theta=rope_theta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_full_transformer_weights(dim, n_layers, n_heads, n_kv_heads, ffn_hidden_dim, vocab_size, seed=42):\n",
    "    \"\"\"Create shared weights for full JAX and PyTorch Transformers\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    head_dim = dim // n_heads\n",
    "    \n",
    "    # Embedding weights\n",
    "    tok_embeddings = np.random.normal(0, 0.02, (vocab_size, dim)).astype(np.float32)\n",
    "    \n",
    "    # Final norm weights\n",
    "    final_norm = np.ones(dim, dtype=np.float32)\n",
    "    \n",
    "    # Create weights for all transformer layers\n",
    "    layer_weights = []\n",
    "    for i in range(n_layers):\n",
    "        np.random.seed(seed + i + 1)  # Different seed for each layer\n",
    "        layer_weights_np = {\n",
    "            # Attention weights\n",
    "            'wq': np.random.normal(0, 0.02, (dim, n_heads * head_dim)).astype(np.float32),\n",
    "            'wk': np.random.normal(0, 0.02, (dim, n_kv_heads * head_dim)).astype(np.float32),\n",
    "            'wv': np.random.normal(0, 0.02, (dim, n_kv_heads * head_dim)).astype(np.float32),\n",
    "            'wo': np.random.normal(0, 0.02, (n_heads * head_dim, dim)).astype(np.float32),\n",
    "            \n",
    "            # Feed-forward weights  \n",
    "            'w1': np.random.normal(0, 0.02, (dim, ffn_hidden_dim)).astype(np.float32),\n",
    "            'w2': np.random.normal(0, 0.02, (ffn_hidden_dim, dim)).astype(np.float32),\n",
    "            'w3': np.random.normal(0, 0.02, (dim, ffn_hidden_dim)).astype(np.float32),\n",
    "            \n",
    "            # Normalization weights\n",
    "            'attention_norm': np.ones(dim, dtype=np.float32),\n",
    "            'ffn_norm': np.ones(dim, dtype=np.float32),\n",
    "        }\n",
    "        layer_weights.append(layer_weights_np)\n",
    "    \n",
    "    return {\n",
    "        'tok_embeddings': tok_embeddings,\n",
    "        'final_norm': final_norm,\n",
    "        'layers': layer_weights\n",
    "    }\n",
    "\n",
    "def set_torch_full_transformer_weights(transformer, weights):\n",
    "    \"\"\"Set weights for PyTorch Transformer\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Set embedding weights\n",
    "        transformer.tok_embeddings.weight.copy_(torch.from_numpy(weights['tok_embeddings']))\n",
    "        \n",
    "        # Set final norm weights\n",
    "        transformer.norm.weight.copy_(torch.from_numpy(weights['final_norm']))\n",
    "        \n",
    "        # Set weights for each layer\n",
    "        for i, layer_weights in enumerate(weights['layers']):\n",
    "            layer = transformer.layers[i]\n",
    "            \n",
    "            # Attention weights (transpose for Linear layers)\n",
    "            layer.attention.wq.weight.copy_(torch.from_numpy(layer_weights['wq'].T))\n",
    "            layer.attention.wk.weight.copy_(torch.from_numpy(layer_weights['wk'].T))\n",
    "            layer.attention.wv.weight.copy_(torch.from_numpy(layer_weights['wv'].T))\n",
    "            layer.attention.wo.weight.copy_(torch.from_numpy(layer_weights['wo'].T))\n",
    "            \n",
    "            # Feed-forward weights\n",
    "            layer.feed_forward.w1.weight.copy_(torch.from_numpy(layer_weights['w1'].T))\n",
    "            layer.feed_forward.w2.weight.copy_(torch.from_numpy(layer_weights['w2'].T))\n",
    "            layer.feed_forward.w3.weight.copy_(torch.from_numpy(layer_weights['w3'].T))\n",
    "            \n",
    "            # Norm weights\n",
    "            layer.attention_norm.weight.copy_(torch.from_numpy(layer_weights['attention_norm']))\n",
    "            layer.ffn_norm.weight.copy_(torch.from_numpy(layer_weights['ffn_norm']))\n",
    "\n",
    "def set_jax_full_transformer_weights(jax_variables, weights, n_heads, n_kv_heads):\n",
    "    \"\"\"Set weights for JAX LLaMa model\"\"\"\n",
    "    head_dim = weights['tok_embeddings'].shape[1] // n_heads\n",
    "    new_params = jax_variables['params'].copy()\n",
    "    \n",
    "    # Set embedding weights\n",
    "    new_params['tok_embeddings'] = {'embedding': jnp.array(weights['tok_embeddings'])}\n",
    "    \n",
    "    # Set final norm weights\n",
    "    new_params['norm_weight'] = jnp.array(weights['final_norm'])\n",
    "    \n",
    "    # Set weights for each layer\n",
    "    for i, layer_weights in enumerate(weights['layers']):\n",
    "        layer_key = f'layer_{i}'\n",
    "        new_params[layer_key] = {\n",
    "            # Attention weights (reshape for JAX format)\n",
    "            'wq': jnp.array(layer_weights['wq'].reshape(-1, n_heads, head_dim)),\n",
    "            'wk': jnp.array(layer_weights['wk'].reshape(-1, n_kv_heads, head_dim)),\n",
    "            'wv': jnp.array(layer_weights['wv'].reshape(-1, n_kv_heads, head_dim)),\n",
    "            'wo': jnp.array(layer_weights['wo']),\n",
    "            \n",
    "            # Feed-forward weights (note the mapping)\n",
    "            'w1_gate': jnp.array(layer_weights['w1']),\n",
    "            'w2_up': jnp.array(layer_weights['w3']),\n",
    "            'w3_down': jnp.array(layer_weights['w2']),\n",
    "            \n",
    "            # Norm weights\n",
    "            'attention_norm_weight': jnp.array(layer_weights['attention_norm']),\n",
    "            'ffn_norm_weight': jnp.array(layer_weights['ffn_norm']),\n",
    "        }\n",
    "    \n",
    "    return {'params': new_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX init\n",
    "key = jax.random.PRNGKey(0)\n",
    "batch_size = 4\n",
    "seq_len = 512\n",
    "dummy_tokens = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n",
    "dummy_kvcache = KVCache.new(n_layers=config.n_layers, bsz=batch_size, max_seq_len=max_seq_len, kv_heads=config.n_kv_heads, head_dim=config.dim // config.n_heads, dtype=config.dtype)\n",
    "\n",
    "transformer_jax = TransformerJax(config)\n",
    "\n",
    "jax_variables = transformer_jax.init(key, dummy_tokens, 0,dummy_kvcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = create_shared_full_transformer_weights(config.dim, config.n_layers, config.n_heads, config.n_kv_heads, config.ffn_hidden_dim, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_variables = set_jax_full_transformer_weights(jax_variables, model_weights, n_heads, n_kv_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = 512\n",
    "\n",
    "input_tokens = np.random.randint(0,vocab_size, (batch_size, seq_len))\n",
    "\n",
    "transformer_torch = TransformerTorch(args)\n",
    "set_torch_full_transformer_weights(transformer_torch, model_weights)\n",
    "\n",
    "torch_output = transformer_torch.forward_inference(torch.tensor(input_tokens,device=device), 0)\n",
    "\n",
    "\n",
    "jax_kvcache = KVCache.new(n_layers=config.n_layers, bsz=batch_size, max_seq_len=max_seq_len, kv_heads=config.n_kv_heads, head_dim=config.dim // config.n_heads, dtype=config.dtype)\n",
    "\n",
    "\n",
    "jax_output,_ = transformer_jax.apply(jax_variables, input_tokens, 0, jax_kvcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 1000])\n",
      "(4, 512, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(torch_output.shape)\n",
    "print(jax_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
