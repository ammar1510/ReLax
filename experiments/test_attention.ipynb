{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch XLA device: xla:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TPU_PROCESS_BOUNDS\"] = \"1,1,1\"\n",
        "os.environ[\"TPU_VISIBLE_CHIPS\"] = \"0\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as jnn\n",
        "import jax.lax as lax\n",
        "from jax import jit\n",
        "from functools import partial\n",
        "from flax import struct\n",
        "from typing import Optional\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "# Try to import torch_xla for TPU support\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    TORCH_XLA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_XLA_AVAILABLE = False\n",
        "\n",
        "# Device selection: TPU > CUDA > CPU\n",
        "if TORCH_XLA_AVAILABLE:\n",
        "    device = torch_xla.device()  # TPU device\n",
        "    print(f\"Using PyTorch XLA device: {device}\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using CUDA device\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU device\")\n",
        "\n",
        "jax_dtype = jnp.bfloat16\n",
        "torch_dtype = torch.bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JAX KVCache implementation\n",
        "@struct.dataclass\n",
        "class KVCache:\n",
        "    k: jax.Array\n",
        "    v: jax.Array\n",
        "    positions: jax.Array  # [bsz] - tracks current filled position for each sequence\n",
        "\n",
        "    @classmethod\n",
        "    def new(\n",
        "        cls,\n",
        "        n_layers: int,\n",
        "        bsz: int,\n",
        "        max_seqlen: int,\n",
        "        kv_heads: int,\n",
        "        head_dim: int,\n",
        "        dtype=jnp.bfloat16,\n",
        "    ) -> \"KVCache\":\n",
        "        return cls(\n",
        "            k=jnp.zeros((n_layers, bsz, max_seqlen, kv_heads, head_dim), dtype=dtype),\n",
        "            v=jnp.zeros((n_layers, bsz, max_seqlen, kv_heads, head_dim), dtype=dtype),\n",
        "            positions=jnp.zeros(bsz, dtype=jnp.int32),\n",
        "        )\n",
        "\n",
        "    def update(self, xk: jax.Array, xv: jax.Array, layer_idx: int):\n",
        "        \"\"\"Updates the Key and Value cache for all sequences, each at their own position.\"\"\"\n",
        "        bsz, seqlen, n_kv_heads, head_dim = xk.shape\n",
        "\n",
        "        # Ensure xk/xv have the same dtype as cache\n",
        "        xk = xk.astype(self.k.dtype)\n",
        "        xv = xv.astype(self.v.dtype)\n",
        "\n",
        "        # Start with current cache\n",
        "        new_k = self.k\n",
        "        new_v = self.v\n",
        "\n",
        "        # Update each sequence at its own position\n",
        "        for i in range(bsz):\n",
        "            start_pos = self.positions[i]\n",
        "            xk_i = xk[i]\n",
        "            xv_i = xv[i]\n",
        "            xk_update = xk_i[None, None, :, :, :]\n",
        "            xv_update = xv_i[None, None, :, :, :]\n",
        "\n",
        "            new_k = jax.lax.dynamic_update_slice(\n",
        "                new_k,\n",
        "                xk_update,\n",
        "                (layer_idx, i, start_pos, 0, 0)\n",
        "            )\n",
        "            new_v = jax.lax.dynamic_update_slice(\n",
        "                new_v,\n",
        "                xv_update,\n",
        "                (layer_idx, i, start_pos, 0, 0)\n",
        "            )\n",
        "\n",
        "        new_positions = self.positions + seqlen\n",
        "        return KVCache(k=new_k, v=new_v, positions=new_positions)\n",
        "\n",
        "    def get_layer(self, layer_idx: int):\n",
        "        \"\"\"Retrieves K/V for a specific layer.\"\"\"\n",
        "        keys = self.k[layer_idx]\n",
        "        values = self.v[layer_idx]\n",
        "        return keys, values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JAX RoPE and helper functions\n",
        "@partial(\n",
        "    jit,\n",
        "    static_argnames=[\n",
        "        \"scale_factor\",\n",
        "        \"low_freq_factor\",\n",
        "        \"high_freq_factor\",\n",
        "        \"old_context_len\",\n",
        "    ],\n",
        "    donate_argnums=[0],\n",
        ")\n",
        "def apply_scaling_jax(\n",
        "    freqs: jax.Array,\n",
        "    scale_factor: float = 8.0,\n",
        "    low_freq_factor: float = 1.0,\n",
        "    high_freq_factor: float = 4.0,\n",
        "    old_context_len: float = 8192.0,\n",
        ") -> jax.Array:\n",
        "    \"\"\"Apply RoPE scaling to frequencies based on Llama 3 implementation.\"\"\"\n",
        "    low_freq_wavelen = old_context_len / low_freq_factor\n",
        "    high_freq_wavelen = old_context_len / high_freq_factor\n",
        "\n",
        "    wavelen = 2 * jnp.pi / freqs\n",
        "\n",
        "    smooth = (old_context_len / wavelen - low_freq_factor) / (\n",
        "        high_freq_factor - low_freq_factor\n",
        "    )\n",
        "    freqs_for_mid_range = (1 - smooth) * freqs / scale_factor + smooth * freqs\n",
        "\n",
        "    new_freqs = jnp.where(\n",
        "        wavelen > low_freq_wavelen,\n",
        "        freqs / scale_factor,\n",
        "        jnp.where(wavelen < high_freq_wavelen, freqs, freqs_for_mid_range),\n",
        "    )\n",
        "    return new_freqs\n",
        "\n",
        "\n",
        "@partial(jit, static_argnames=[\"head_dim\", \"end\", \"use_scaled\", \"dtype\"])\n",
        "def precompute_freqs_cis_jax(\n",
        "    head_dim: int,\n",
        "    end: int,\n",
        "    theta: float = 500000.0,\n",
        "    use_scaled: bool = False,\n",
        "    dtype: jnp.dtype = jnp.float32,\n",
        ") -> jax.Array:\n",
        "    \"\"\"Precompute the rotational frequency embeddings.\"\"\"\n",
        "    freqs = 1.0 / (\n",
        "        theta\n",
        "        ** (jnp.arange(0, head_dim, 2)[: (head_dim // 2)].astype(dtype) / head_dim)\n",
        "    )\n",
        "    if use_scaled:\n",
        "        freqs = apply_scaling_jax(freqs)\n",
        "    t = jnp.arange(end, dtype=dtype)\n",
        "    freqs = jnp.outer(t, freqs)\n",
        "\n",
        "    freqs_cos = jnp.cos(freqs)\n",
        "    freqs_sin = jnp.sin(freqs)\n",
        "\n",
        "    freqs_cis = jnp.stack([freqs_cos, freqs_sin], axis=-1)\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "@partial(jit, donate_argnums=[0])\n",
        "def apply_rotary_emb_batch(x: jax.Array, freqs_cis: jax.Array) -> jax.Array:\n",
        "    \"\"\"Apply Rotary Positional Embeddings (RoPE) to a tensor with per-batch-item frequencies.\"\"\"\n",
        "    x_shaped = x.reshape(*x.shape[:-1], -1, 2)\n",
        "    x_r, x_i = x_shaped[..., 0], x_shaped[..., 1]\n",
        "\n",
        "    freqs_cis = freqs_cis[:, :, None, :, :]\n",
        "    freqs_cos, freqs_sin = freqs_cis[..., 0], freqs_cis[..., 1]\n",
        "\n",
        "    x_out_r = x_r * freqs_cos - x_i * freqs_sin\n",
        "    x_out_i = x_r * freqs_sin + x_i * freqs_cos\n",
        "\n",
        "    x_out = jnp.stack([x_out_r, x_out_i], axis=-1).reshape(x.shape)\n",
        "    return x_out\n",
        "\n",
        "\n",
        "@partial(jit, static_argnames=[\"n_rep\"])\n",
        "def repeat_kv(x: jax.Array, n_rep: int) -> jax.Array:\n",
        "    \"\"\"Repeat Key/Value heads for Grouped Query Attention.\"\"\"\n",
        "    bs, slen, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return jnp.broadcast_to(\n",
        "        x[:, :, :, None, :], (bs, slen, n_kv_heads, n_rep, head_dim)\n",
        "    ).reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JAX Attention implementation\n",
        "@struct.dataclass\n",
        "class AttentionParams:\n",
        "    wq: jax.Array\n",
        "    wk: jax.Array\n",
        "    wv: jax.Array\n",
        "    wo: jax.Array\n",
        "\n",
        "\n",
        "@partial(jit, static_argnames=[\"layer_idx\"], donate_argnums=[0, 1, 3])\n",
        "def grouped_query_attention(\n",
        "    x: jax.Array,\n",
        "    freqs_cis: jax.Array,\n",
        "    params: AttentionParams,\n",
        "    kv_cache: KVCache,\n",
        "    layer_idx: int,\n",
        "    seq_lengths: jax.Array,\n",
        ") -> tuple[jax.Array, KVCache]:\n",
        "    \"\"\"Compute Grouped Query Attention with variable-length sequences.\"\"\"\n",
        "    bsz, seqlen, dim = x.shape\n",
        "\n",
        "    # Get per-sequence cache positions\n",
        "    start_positions = kv_cache.positions\n",
        "\n",
        "    # Project inputs to queries, keys, values\n",
        "    xq = jnp.einsum(\"bsd,dhc->bshc\", x, params.wq)\n",
        "    xk = jnp.einsum(\"bsd,dkc->bskc\", x, params.wk)\n",
        "    xv = jnp.einsum(\"bsd,dvc->bsvc\", x, params.wv)\n",
        "\n",
        "    # Apply RoPE at absolute positions\n",
        "    position_offsets = jnp.arange(seqlen)[None, :]\n",
        "    absolute_positions = start_positions[:, None] + position_offsets\n",
        "\n",
        "    # Get frequencies for these absolute positions\n",
        "    batch_freqs_cis = freqs_cis[absolute_positions]\n",
        "\n",
        "    # Apply rotary embeddings\n",
        "    xq = apply_rotary_emb_batch(xq, batch_freqs_cis)\n",
        "    xk = apply_rotary_emb_batch(xk, batch_freqs_cis)\n",
        "\n",
        "    # Update cache\n",
        "    updated_cache = kv_cache.update(xk, xv, layer_idx)\n",
        "    keys, values = updated_cache.get_layer(layer_idx)\n",
        "\n",
        "    max_seqlen = keys.shape[1]\n",
        "\n",
        "    # Build per-sequence attention masks\n",
        "    def build_mask_for_sequence(true_len, cache_pos):\n",
        "        query_offsets = jnp.arange(seqlen)\n",
        "        key_positions = jnp.arange(max_seqlen)\n",
        "        query_positions = (cache_pos - seqlen) + query_offsets\n",
        "\n",
        "        causal_mask = query_positions[:, None] >= key_positions[None, :]\n",
        "        valid_query_mask = query_offsets < true_len\n",
        "\n",
        "        mask = causal_mask & valid_query_mask[:, None]\n",
        "        return mask\n",
        "\n",
        "    # Apply to all sequences\n",
        "    mask = jax.vmap(build_mask_for_sequence)(seq_lengths, updated_cache.positions)\n",
        "    mask = mask[:, None, :, :]\n",
        "\n",
        "    # Perform attention\n",
        "    attn_output = jnn.dot_product_attention(\n",
        "        query=xq,\n",
        "        key=keys,\n",
        "        value=values,\n",
        "        mask=mask,\n",
        "    )\n",
        "\n",
        "    # Output projection\n",
        "    attn_output = attn_output.reshape(bsz, seqlen, -1)\n",
        "    output = jnp.einsum(\"bsd,do->bso\", attn_output, params.wo)\n",
        "\n",
        "    return output, updated_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch ModelArgs and helper functions\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 4096\n",
        "    n_layers: int = 32\n",
        "    n_heads: int = 32\n",
        "    n_kv_heads: Optional[int] = None\n",
        "    flash: bool = False\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if hasattr(self, k):\n",
        "                setattr(self, k, v)\n",
        "        if self.n_kv_heads is None:\n",
        "            self.n_kv_heads = self.n_heads\n",
        "\n",
        "\n",
        "def apply_scaling_torch(freqs: torch.Tensor):\n",
        "    \"\"\"Apply RoPE scaling based on Llama 3 implementation.\"\"\"\n",
        "    scale_factor = 8\n",
        "    low_freq_factor = 1\n",
        "    high_freq_factor = 4\n",
        "    old_context_len = 8192\n",
        "    low_freq_wavelen = old_context_len / low_freq_factor\n",
        "    high_freq_wavelen = old_context_len / high_freq_factor\n",
        "    new_freqs = []\n",
        "    for freq in freqs:\n",
        "        wavelen = 2 * math.pi / freq\n",
        "        if wavelen < high_freq_wavelen:\n",
        "            new_freqs.append(freq)\n",
        "        elif wavelen > low_freq_wavelen:\n",
        "            new_freqs.append(freq / scale_factor)\n",
        "        else:\n",
        "            assert low_freq_wavelen != high_freq_wavelen\n",
        "            smooth = (old_context_len / wavelen - low_freq_factor) / (\n",
        "                high_freq_factor - low_freq_factor\n",
        "            )\n",
        "            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n",
        "    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n",
        "\n",
        "\n",
        "def precompute_freqs_cis_torch(\n",
        "    dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False\n",
        "):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    if use_scaled:\n",
        "        \n",
        "        freqs = apply_scaling_torch(freqs)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    freqs_cis_real = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n",
        "    return freqs_cis_real\n",
        "\n",
        "\n",
        "def apply_rotary_emb_torch(x, freqs_cis):\n",
        "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
        "    x_out2 = torch.stack(\n",
        "        [\n",
        "            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
        "            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
        "        ],\n",
        "        -1,\n",
        "    )\n",
        "    x_out2 = x_out2.flatten(3)\n",
        "    return x_out2.type_as(x)\n",
        "\n",
        "\n",
        "def repeat_kv_torch(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
        "    bs, slen, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch KVCache and Attention implementation\n",
        "class KVCache_torch(nn.Module):\n",
        "    def __init__(self, batch_size, seq_length, n_kv_heads, head_dim, dtype, device):\n",
        "        super().__init__()\n",
        "        cache_shape = (batch_size, seq_length, n_kv_heads, head_dim)\n",
        "        self.register_buffer(\n",
        "            \"cache_k\", torch.zeros(cache_shape, dtype=dtype, device=device)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"cache_v\", torch.zeros(cache_shape, dtype=dtype, device=device)\n",
        "        )\n",
        "\n",
        "    def update(self, start_pos, xk, xv):\n",
        "        seqlen = xk.size(1)\n",
        "        self.cache_k[:, start_pos : start_pos + seqlen] = xk\n",
        "        self.cache_v[:, start_pos : start_pos + seqlen] = xv\n",
        "        xk = self.cache_k[:, : start_pos + seqlen]\n",
        "        xv = self.cache_v[:, : start_pos + seqlen]\n",
        "        return xk, xv\n",
        "\n",
        "\n",
        "class Attention_torch(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.flash = args.flash\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        model_parallel_size = 1\n",
        "        self.n_local_heads = args.n_heads // model_parallel_size\n",
        "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
        "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        start_pos: int,\n",
        "        freqs_cis: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "    ):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
        "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
        "        \n",
        "        xq = apply_rotary_emb_torch(xq, freqs_cis)\n",
        "        xk = apply_rotary_emb_torch(xk, freqs_cis)\n",
        "        \n",
        "        if self.cache is not None:\n",
        "            xk, xv = self.cache.update(start_pos, xk, xv)\n",
        "        \n",
        "        xk = repeat_kv_torch(xk, self.n_rep)\n",
        "        xv = repeat_kv_torch(xv, self.n_rep)\n",
        "        \n",
        "        xq, xk, xv = (x.transpose(1, 2) for x in (xq, xk, xv))\n",
        "        \n",
        "        if self.flash:\n",
        "            output = F.scaled_dot_product_attention(xq, xk, xv, mask)\n",
        "        else:\n",
        "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "            if mask is not None:\n",
        "                scores = scores + mask\n",
        "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "            output = torch.matmul(scores, xv)\n",
        "        \n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
        "        proj = self.wo(output)\n",
        "        return proj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 4\n",
            "Sequence length: 128\n",
            "Model dimension: 2048\n",
            "Number of heads: 16\n",
            "Number of KV heads: 4\n",
            "Head dimension: 128\n",
            "Cache sequence length: 64\n",
            "Start position: 64\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup Parameters\n",
        "bsz = 4\n",
        "seqlen = 128\n",
        "dim = 2048\n",
        "n_heads = 16\n",
        "n_kv_heads = 4\n",
        "head_dim = dim // n_heads\n",
        "cache_seq_len = 64\n",
        "start_pos = cache_seq_len\n",
        "dtype = np.float32\n",
        "max_seq_len = 1024\n",
        "n_layers = 1\n",
        "\n",
        "print(f\"Batch size: {bsz}\")\n",
        "print(f\"Sequence length: {seqlen}\")\n",
        "print(f\"Model dimension: {dim}\")\n",
        "print(f\"Number of heads: {n_heads}\")\n",
        "print(f\"Number of KV heads: {n_kv_heads}\")\n",
        "print(f\"Head dimension: {head_dim}\")\n",
        "print(f\"Cache sequence length: {cache_seq_len}\")\n",
        "print(f\"Start position: {start_pos}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using scaled RoPE: True\n",
            "Input shape: (4, 128, 2048)\n",
            "Weight shapes: wq=(2048, 2048), wk=(2048, 512), wv=(2048, 512), wo=(2048, 2048)\n",
            "Frequencies shape: (1024, 64, 2)\n"
          ]
        }
      ],
      "source": [
        "# 2. Create inputs and parameters\n",
        "use_scaled_rope = True  # Enable scaled RoPE\n",
        "\n",
        "np.random.seed(42)\n",
        "x_np = np.random.randn(bsz, seqlen, dim).astype(dtype)\n",
        "\n",
        "# Create weight matrices\n",
        "wq_np = np.random.normal(0, 0.02, (dim, n_heads * head_dim)).astype(dtype)\n",
        "wk_np = np.random.normal(0, 0.02, (dim, n_kv_heads * head_dim)).astype(dtype)\n",
        "wv_np = np.random.normal(0, 0.02, (dim, n_kv_heads * head_dim)).astype(dtype)\n",
        "wo_np = np.random.normal(0, 0.02, (n_heads * head_dim, dim)).astype(dtype)\n",
        "\n",
        "# Precompute frequencies with scaled RoPE\n",
        "print(f\"Using scaled RoPE: {use_scaled_rope}\")\n",
        "freqs_cis_torch = precompute_freqs_cis_torch(head_dim, max_seq_len, use_scaled=use_scaled_rope).to(dtype=torch_dtype)\n",
        "freqs_cis_jax = jnp.array(freqs_cis_torch.float().detach().cpu().numpy(), dtype=jax_dtype)\n",
        "\n",
        "print(f\"Input shape: {x_np.shape}\")\n",
        "print(f\"Weight shapes: wq={wq_np.shape}, wk={wk_np.shape}, wv={wv_np.shape}, wo={wo_np.shape}\")\n",
        "print(f\"Frequencies shape: {freqs_cis_jax.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX KV cache initialized with shape k=(1, 4, 1024, 4, 128), v=(1, 4, 1024, 4, 128)\n"
          ]
        }
      ],
      "source": [
        "# 3. JAX setup\n",
        "x_jax = jnp.array(x_np, dtype=jax_dtype)\n",
        "jax_params = AttentionParams(\n",
        "    wq=jnp.array(wq_np, dtype=jax_dtype).reshape(dim, n_heads, head_dim),\n",
        "    wk=jnp.array(wk_np, dtype=jax_dtype).reshape(dim, n_kv_heads, head_dim),\n",
        "    wv=jnp.array(wv_np, dtype=jax_dtype).reshape(dim, n_kv_heads, head_dim),\n",
        "    wo=jnp.array(wo_np, dtype=jax_dtype),\n",
        ")\n",
        "\n",
        "# Initialize KV cache with randomly initialized values up to cache_seq_len\n",
        "kv_cache_jax = KVCache.new(\n",
        "    n_layers, bsz, max_seq_len, n_kv_heads, head_dim, dtype=jax_dtype\n",
        ")\n",
        "\n",
        "# Pre-fill cache with random values\n",
        "prefill_k = np.random.randn(n_layers, bsz, cache_seq_len, n_kv_heads, head_dim).astype(dtype)\n",
        "prefill_v = np.random.randn(n_layers, bsz, cache_seq_len, n_kv_heads, head_dim).astype(dtype)\n",
        "k_init_jax = jnp.zeros((n_layers, bsz, max_seq_len, n_kv_heads, head_dim), dtype=jax_dtype)\n",
        "v_init_jax = jnp.zeros((n_layers, bsz, max_seq_len, n_kv_heads, head_dim), dtype=jax_dtype)\n",
        "k_updated_jax = k_init_jax.at[:, :, :cache_seq_len, :, :].set(jnp.array(prefill_k, dtype=jax_dtype))\n",
        "v_updated_jax = v_init_jax.at[:, :, :cache_seq_len, :, :].set(jnp.array(prefill_v, dtype=jax_dtype))\n",
        "kv_cache_jax = KVCache(\n",
        "    k=k_updated_jax,\n",
        "    v=v_updated_jax,\n",
        "    positions=jnp.full((bsz,), cache_seq_len, dtype=jnp.int32)\n",
        ")\n",
        "\n",
        "print(f\"JAX KV cache initialized with shape k={kv_cache_jax.k.shape}, v={kv_cache_jax.v.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch KV cache initialized\n"
          ]
        }
      ],
      "source": [
        "# 4. PyTorch setup\n",
        "model_args = ModelArgs(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, flash=False)\n",
        "x_torch = torch.tensor(x_np, device=device, dtype=torch_dtype)\n",
        "torch_attention = Attention_torch(model_args)\n",
        "torch_attention.wq.weight = nn.Parameter(torch.tensor(wq_np.T, device=device, dtype=torch_dtype))\n",
        "torch_attention.wk.weight = nn.Parameter(torch.tensor(wk_np.T, device=device, dtype=torch_dtype))\n",
        "torch_attention.wv.weight = nn.Parameter(torch.tensor(wv_np.T, device=device, dtype=torch_dtype))\n",
        "torch_attention.wo.weight = nn.Parameter(torch.tensor(wo_np.T, device=device, dtype=torch_dtype))\n",
        "\n",
        "kv_cache_torch = KVCache_torch(\n",
        "    bsz, max_seq_len, n_kv_heads, head_dim, dtype=torch_dtype, device=device\n",
        ")\n",
        "\n",
        "# Pre-fill PyTorch cache with same random values\n",
        "kv_cache_torch.cache_k[:, :cache_seq_len, :, :] = torch.tensor(prefill_k[0], device=device, dtype=torch_dtype)\n",
        "kv_cache_torch.cache_v[:, :cache_seq_len, :, :] = torch.tensor(prefill_v[0], device=device, dtype=torch_dtype)\n",
        "torch_attention.cache = kv_cache_torch\n",
        "\n",
        "print(f\"PyTorch KV cache initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ammar/ReLax/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1280: UserWarning: Some donated buffers were not usable: bfloat16[1024,64,2].\n",
            "See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "  warnings.warn(\"Some donated buffers were not usable:\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX output shape: (4, 128, 2048)\n",
            "JAX output dtype: bfloat16\n"
          ]
        }
      ],
      "source": [
        "# 5. Execute JAX attention\n",
        "seq_lengths = jnp.full((bsz,), seqlen, dtype=jnp.int32)\n",
        "output_jax, updated_kv_cache_jax = grouped_query_attention(\n",
        "    x_jax, freqs_cis_jax, jax_params, kv_cache_jax, 0, seq_lengths\n",
        ")\n",
        "\n",
        "print(f\"JAX output shape: {output_jax.shape}\")\n",
        "print(f\"JAX output dtype: {output_jax.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch output shape: torch.Size([4, 128, 2048])\n",
            "PyTorch output dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "# 6. Execute PyTorch attention\n",
        "freqs_cis_torch_sliced = freqs_cis_torch[start_pos : start_pos + seqlen].to(device=device)\n",
        "\n",
        "# Create causal mask for PyTorch\n",
        "mask = None\n",
        "if seqlen > 1:\n",
        "    mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=device)\n",
        "    mask = torch.triu(mask, diagonal=1)\n",
        "    mask = torch.hstack(\n",
        "        [torch.zeros((seqlen, start_pos), device=device), mask]\n",
        "    ).type_as(x_torch)\n",
        "\n",
        "output_torch = torch_attention.forward(\n",
        "    x_torch, start_pos, freqs_cis_torch_sliced, mask\n",
        ")\n",
        "\n",
        "print(f\"PyTorch output shape: {output_torch.shape}\")\n",
        "print(f\"PyTorch output dtype: {output_torch.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Attention output test passed!\n",
            "Max absolute difference: 0.0039016902446746826\n",
            "Mean absolute difference: 0.0004866586241405457\n"
          ]
        }
      ],
      "source": [
        "# 7. Compare outputs\n",
        "output_jax_np = np.array(output_jax)\n",
        "output_torch_np = output_torch.float().detach().cpu().numpy()\n",
        "\n",
        "# Check shapes match\n",
        "assert output_jax_np.shape == output_torch_np.shape, f\"Shape mismatch: JAX {output_jax_np.shape} vs PyTorch {output_torch_np.shape}\"\n",
        "\n",
        "# Check that output is not all zeros or NaN\n",
        "assert not np.all(output_jax_np == 0), \"JAX output should not be all zeros\"\n",
        "assert not np.any(np.isnan(output_jax_np)), \"JAX output should not contain NaN values\"\n",
        "assert not np.all(output_torch_np == 0), \"PyTorch output should not be all zeros\"\n",
        "assert not np.any(np.isnan(output_torch_np)), \"PyTorch output should not contain NaN values\"\n",
        "\n",
        "# Compare with same tolerances as test_ops.py\n",
        "np.testing.assert_allclose(\n",
        "    output_jax_np, output_torch_np, rtol=5e-3, atol=5e-3\n",
        ")\n",
        "\n",
        "print(\"✓ Attention output test passed!\")\n",
        "print(f\"Max absolute difference: {np.max(np.abs(output_jax_np - output_torch_np))}\")\n",
        "print(f\"Mean absolute difference: {np.mean(np.abs(output_jax_np - output_torch_np))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "\nNot equal to tolerance rtol=1e-05, atol=1e-05\n\nMismatched elements: 80122 / 2097152 (3.82%)\nMax absolute difference among violations: 0.03125\nMax relative difference among violations: 227.83545\n ACTUAL: array([[[[-1.09375, -0.796875, 0.746094, ..., -0.275391, -1.01562,\n          -0.9375],\n         [-1.01562, 0.292969, 1.08594, ..., 0.388672, 0.0966797,...\n DESIRED: array([[[[-1.09375 , -0.796875,  0.746094, ..., -0.275391, -1.015625,\n          -0.9375  ],\n         [-1.015625,  0.292969,  1.085938, ...,  0.388672,  0.09668 ,...",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(np.array(updated_v_jax[:, cache_seq_len:cache_seq_len+seqlen, :, :]) == \u001b[32m0\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mJAX new values should be cached\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Compare cache values\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_k_jax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdated_k_torch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m np.testing.assert_allclose(\n\u001b[32m     21\u001b[39m     np.array(updated_v_jax),\n\u001b[32m     22\u001b[39m     updated_v_torch.float().detach().cpu().numpy(),\n\u001b[32m     23\u001b[39m     rtol=\u001b[32m1e-5\u001b[39m,\n\u001b[32m     24\u001b[39m     atol=\u001b[32m1e-4\u001b[39m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ KV cache test passed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ReLax/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:926\u001b[39m, in \u001b[36massert_array_compare\u001b[39m\u001b[34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\u001b[39m\n\u001b[32m    921\u001b[39m         err_msg += \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join(remarks)\n\u001b[32m    922\u001b[39m         msg = build_err_msg([ox, oy], err_msg,\n\u001b[32m    923\u001b[39m                             verbose=verbose, header=header,\n\u001b[32m    924\u001b[39m                             names=names,\n\u001b[32m    925\u001b[39m                             precision=precision)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    928\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n",
            "\u001b[31mAssertionError\u001b[39m: \nNot equal to tolerance rtol=1e-05, atol=1e-05\n\nMismatched elements: 80122 / 2097152 (3.82%)\nMax absolute difference among violations: 0.03125\nMax relative difference among violations: 227.83545\n ACTUAL: array([[[[-1.09375, -0.796875, 0.746094, ..., -0.275391, -1.01562,\n          -0.9375],\n         [-1.01562, 0.292969, 1.08594, ..., 0.388672, 0.0966797,...\n DESIRED: array([[[[-1.09375 , -0.796875,  0.746094, ..., -0.275391, -1.015625,\n          -0.9375  ],\n         [-1.015625,  0.292969,  1.085938, ...,  0.388672,  0.09668 ,..."
          ]
        }
      ],
      "source": [
        "# 8. Compare KV caches\n",
        "updated_k_jax = updated_kv_cache_jax.k[0]\n",
        "updated_v_jax = updated_kv_cache_jax.v[0]\n",
        "updated_k_torch = torch_attention.cache.cache_k\n",
        "updated_v_torch = torch_attention.cache.cache_v\n",
        "\n",
        "# Check KV cache was updated correctly\n",
        "assert not np.all(np.array(updated_k_jax[:, :cache_seq_len, :, :]) == 0), \"JAX pre-filled keys should remain in cache\"\n",
        "assert not np.all(np.array(updated_v_jax[:, :cache_seq_len, :, :]) == 0), \"JAX pre-filled values should remain in cache\"\n",
        "assert not np.all(np.array(updated_k_jax[:, cache_seq_len:cache_seq_len+seqlen, :, :]) == 0), \"JAX new keys should be cached\"\n",
        "assert not np.all(np.array(updated_v_jax[:, cache_seq_len:cache_seq_len+seqlen, :, :]) == 0), \"JAX new values should be cached\"\n",
        "\n",
        "# Compare cache values\n",
        "np.testing.assert_allclose(\n",
        "    np.array(updated_k_jax),\n",
        "    updated_k_torch.float().detach().cpu().numpy(),\n",
        "    rtol=1e-5,\n",
        "    atol=1e-5,\n",
        ")\n",
        "np.testing.assert_allclose(\n",
        "    np.array(updated_v_jax),\n",
        "    updated_v_torch.float().detach().cpu().numpy(),\n",
        "    rtol=1e-5,\n",
        "    atol=1e-4,\n",
        ")\n",
        "\n",
        "print(\"✓ KV cache test passed!\")\n",
        "print(f\"Max key difference: {np.max(np.abs(np.array(updated_k_jax) - updated_k_torch.float().detach().cpu().numpy()))}\")\n",
        "print(f\"Max value difference: {np.max(np.abs(np.array(updated_v_jax) - updated_v_torch.float().detach().cpu().numpy()))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
