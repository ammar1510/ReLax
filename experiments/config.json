{"hidden_size": 768, "num_hidden_layers": 4, "num_attention_heads": 12, "num_key_value_heads": 4, "intermediate_size": 2048, "vocab_size": 1000, "rms_norm_eps": 1e-06, "rope_theta": 10000.0, "max_position_embeddings": 2048, "hidden_act": "silu", "head_dim": 64}