{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from experiments.torch_llama import Transformer\n",
    "from experiments.torch_llama import ModelArgs\n",
    "from models.llama.tokenizer import Tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash = False\n",
    "max_seq_len = 1024\n",
    "max_batch_size = 8\n",
    "\n",
    "args = ModelArgs(\n",
    "    dim=768,\n",
    "    n_layers=8,\n",
    "    n_heads=12,\n",
    "    n_kv_heads=4,\n",
    "    ffn_hidden_dim=2048,\n",
    "    vocab_size=1000,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    rope_theta=500000,\n",
    "    use_scaled_rope=False,\n",
    "    max_batch_size=4,\n",
    ")\n",
    "\n",
    "ckpt_dir = \"/workspace/ReLax/artifacts/weights/Llama-3.2-3B/original\"\n",
    "tokenizer_path = \"/workspace/ReLax/artifacts/weights/Llama-3.2-3B/original/tokenizer.model\""
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust the import path based on your project structure\n",
    "# This assumes 'tests' is at the same level as 'models'\n",
    "from models.llama.config import ModelConfig\n",
    "\n",
    "def small_model_config_dir(tmp_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Creates a temporary directory with a config.json for a small model.\n",
    "    Returns the path to the directory as a string.\n",
    "    \"\"\"\n",
    "    config_data = {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_hidden_layers\": 2,\n",
    "        \"num_attention_heads\": 4,\n",
    "        \"num_key_value_heads\": 2,\n",
    "        \"intermediate_size\": 128,\n",
    "        \"vocab_size\": 1000,\n",
    "        \"rms_norm_eps\": 1e-6,\n",
    "        \"rope_theta\": 1000.0,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"hidden_act\": \"silu\"\n",
    "    }\n",
    "    \n",
    "    config_path = tmp_path / \"config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_data, f)\n",
    "        \n",
    "    return str(tmp_path)\n",
    "\n",
    "def test_model_config_loading(small_model_config_dir: str):\n",
    "    \"\"\"\n",
    "    Tests loading a model configuration from a JSON file.\n",
    "    \"\"\"\n",
    "    # Load the configuration using the class method\n",
    "    config = ModelConfig.from_json_file(small_model_config_dir)\n",
    "\n",
    "    # Assert that all attributes are loaded correctly\n",
    "    assert config.dim == 64\n",
    "    assert config.n_layers == 2\n",
    "    assert config.n_heads == 4\n",
    "    assert config.n_kv_heads == 2\n",
    "    assert config.ffn_hidden_dim == 128\n",
    "    assert config.vocab_size == 1000\n",
    "    assert config.rms_norm_eps == 1e-6\n",
    "    assert config.rope_theta == 1000.0\n",
    "    assert config.max_seq_len == 512\n",
    "    assert config.activation_fn == \"silu\"\n",
    "\n",
    "    # Assert that the calculated property is correct\n",
    "    assert config.head_dim == 16 # (64 / 4)\n",
    "\n",
    "def test_gqa_validation():\n",
    "    \"\"\"\n",
    "    Tests that the GQA constraint (n_heads % n_kv_heads == 0) is enforced.\n",
    "    \"\"\"\n",
    "    with pytest.raises(ValueError, match=\"must be divisible by\"):\n",
    "        # This configuration is invalid and should raise an error\n",
    "        ModelConfig(n_heads=5, n_kv_heads=2)\n"
>>>>>>> bb00f11 (Add model_exp.ipynb)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "ckpt_path = checkpoints[0]\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "    params = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            flash=flash,\n",
    "            **params,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "model = Transformer(model_args).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['output.weight'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor([tokenizer.encode(\"Hello, world!\", bos=False, eos=False)],device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ReLax/experiments/torch_llama.py:287\u001b[39m, in \u001b[36mTransformer.forward_inference\u001b[39m\u001b[34m(self, tokens, start_pos)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: torch.Tensor, start_pos: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    285\u001b[39m     \u001b[38;5;66;03m# for use during inference\u001b[39;00m\n\u001b[32m    286\u001b[39m     _bsz, seqlen = tokens.shape\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtok_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mself\u001b[39m.freqs_cis = \u001b[38;5;28mself\u001b[39m.freqs_cis.to(h.device)\n\u001b[32m    289\u001b[39m     freqs_cis = \u001b[38;5;28mself\u001b[39m.freqs_cis[start_pos : start_pos + seqlen]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ReLax/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ReLax/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ReLax/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ReLax/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "logits = model.forward_inference(tokens, 0)"
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig.from_json_file(\"/Users/ammar3.shaikh/Desktop/ReLax/experiments\")"
>>>>>>> bb00f11 (Add model_exp.ipynb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llama.config import ModelConfig\n",
    "from models.llama.model import LLaMA\n",
    "from utils.kvcache import KVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLaMA(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul got incompatible shapes for broadcasting: (1, 10, 2, 8), (1, 10, 4, 8).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m start_pos = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m kvcache = KVCache.new(n_layers=\u001b[32m2\u001b[39m, bsz=\u001b[32m1\u001b[39m, max_seq_len=\u001b[32m1024\u001b[39m, kv_heads=\u001b[32m2\u001b[39m, head_dim=\u001b[32m16\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m params = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkvcache\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 9 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/models/llama/model.py:96\u001b[39m, in \u001b[36mLLaMA.__call__\u001b[39m\u001b[34m(self, tokens, start_pos, kv_cache)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Transformer layers\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     h, kv_cache = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Final normalization and output projection\u001b[39;00m\n\u001b[32m     99\u001b[39m h = rms_norm(h, \u001b[38;5;28mself\u001b[39m.norm_weight, eps=\u001b[38;5;28mself\u001b[39m.args.rms_norm_eps)\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/models/llama/model.py:35\u001b[39m, in \u001b[36mTransformerBlock.__call__\u001b[39m\u001b[34m(self, x, freqs_cis, kv_cache, layer_idx, start_pos)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: jax.Array, freqs_cis: jax.Array, kv_cache: KVCache, layer_idx: \u001b[38;5;28mint\u001b[39m, start_pos: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[jax.Array, KVCache]:\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Attention block\u001b[39;00m\n\u001b[32m     34\u001b[39m     h_norm = rms_norm(x, \u001b[38;5;28mself\u001b[39m.attention_norm_weight, eps=\u001b[38;5;28mself\u001b[39m.args.rms_norm_eps)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     attn_output, kv_cache = \u001b[43mgrouped_query_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mh_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_kv_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     x = x + attn_output  \u001b[38;5;66;03m# Residual connection\u001b[39;00m\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Feed-forward block\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/utils/ops.py:184\u001b[39m, in \u001b[36mgrouped_query_attention\u001b[39m\u001b[34m(x, freqs_cis, params, kv_cache, layer_idx, start_pos, n_heads, n_kv_heads)\u001b[39m\n\u001b[32m    179\u001b[39m current_freqs_cis = (\n\u001b[32m    180\u001b[39m     lax.dynamic_slice_in_dim(freqs_cis[\u001b[32m0\u001b[39m], start_pos, seqlen, axis=\u001b[32m0\u001b[39m),\n\u001b[32m    181\u001b[39m     lax.dynamic_slice_in_dim(freqs_cis[\u001b[32m1\u001b[39m], start_pos, seqlen, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    182\u001b[39m )\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Pass the sliced freqs to RoPE\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m xq, xk = \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_freqs_cis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Update the KV cache\u001b[39;00m\n\u001b[32m    187\u001b[39m updated_cache = kv_cache.update(xk, xv, layer_idx, start_pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/utils/ops.py:108\u001b[39m, in \u001b[36mapply_rotary_emb\u001b[39m\u001b[34m(xq, xk, freqs_cis)\u001b[39m\n\u001b[32m    106\u001b[39m xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n\u001b[32m    107\u001b[39m xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m xk_out_r = \u001b[43mxk_r\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cos\u001b[49m - xk_i * freqs_sin\n\u001b[32m    109\u001b[39m xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Combine back\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/.venv/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:583\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    581\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/.venv/lib/python3.12/site-packages/jax/_src/numpy/ufunc_api.py:180\u001b[39m, in \u001b[36mufunc.__call__\u001b[39m\u001b[34m(self, out, where, *args)\u001b[39m\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m call = \u001b[38;5;28mself\u001b[39m.__static_props[\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_vectorized\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/.venv/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:1280\u001b[39m, in \u001b[36mmultiply\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m   1254\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Multiply two arrays element-wise.\u001b[39;00m\n\u001b[32m   1255\u001b[39m \n\u001b[32m   1256\u001b[39m \u001b[33;03mJAX implementation of :obj:`numpy.multiply`. This is a universal function,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1277\u001b[39m \u001b[33;03m  Array([ 0, 10, 20, 30], dtype=int32)\u001b[39;00m\n\u001b[32m   1278\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1279\u001b[39m x, y = promote_args(\u001b[33m\"\u001b[39m\u001b[33mmultiply\u001b[39m\u001b[33m\"\u001b[39m, x, y)\n\u001b[32m-> \u001b[39m\u001b[32m1280\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x.dtype != \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax.bitwise_and(x, y)\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/.venv/lib/python3.12/site-packages/jax/_src/lax/lax.py:135\u001b[39m, in \u001b[36m_try_broadcast_shapes\u001b[39m\u001b[34m(name, *shapes)\u001b[39m\n\u001b[32m    133\u001b[39m       result_shape.append(non_1s[\u001b[32m0\u001b[39m])\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got incompatible shapes for broadcasting: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    136\u001b[39m                       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[31mTypeError\u001b[39m: mul got incompatible shapes for broadcasting: (1, 10, 2, 8), (1, 10, 4, 8)."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "rng = jax.random.PRNGKey(0)\n",
    "tokens = jax.random.randint(rng, (1, 10), 0, 128256)\n",
    "start_pos = 0\n",
    "kvcache = KVCache.new(n_layers=2, bsz=1, max_seq_len=1024, kv_heads=2, head_dim=16)\n",
    "params = model.init(rng, tokens, start_pos, kvcache)"
   ]
>>>>>>> bb00f11 (Add model_exp.ipynb)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.12.11"
=======
   "version": "3.12.8"
>>>>>>> bb00f11 (Add model_exp.ipynb)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
