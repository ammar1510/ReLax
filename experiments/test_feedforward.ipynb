{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "from functools import partial\n",
        "from flax import struct\n",
        "from typing import Optional\n",
        "\n",
        "# Try to import torch_xla for TPU support\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    TORCH_XLA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_XLA_AVAILABLE = False\n",
        "\n",
        "# Device selection: TPU > CUDA > CPU\n",
        "if TORCH_XLA_AVAILABLE:\n",
        "    device = torch_xla.device()  # TPU device\n",
        "    print(f\"Using PyTorch XLA device: {device}\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using CUDA device\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU device\")\n",
        "\n",
        "jax_dtype = jnp.bfloat16\n",
        "torch_dtype = torch.bfloat16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JAX FeedForward implementation\n",
        "@struct.dataclass\n",
        "class FeedForwardParams:\n",
        "    w_gate: jax.Array  # Corresponds to gate_proj\n",
        "    w_up: jax.Array  # Corresponds to up_proj\n",
        "    w_down: jax.Array  # Corresponds to down_proj\n",
        "\n",
        "\n",
        "@partial(jit, static_argnames=[\"activation_fn\"], donate_argnums=[0])\n",
        "def feed_forward(\n",
        "    x: jax.Array,\n",
        "    params: FeedForwardParams,\n",
        "    activation_fn: str,  # Added activation function name\n",
        ") -> jax.Array:\n",
        "    \"\"\"\n",
        "    Compute FeedForward network (MLP) using a configurable activation function (like SwiGLU).\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor of shape [batch_size, seqlen, dim].\n",
        "        params: Dataclass containing weight matrices (w_gate, w_up, w_down).\n",
        "        activation_fn: Name of the activation function ('silu', 'relu', 'gelu').\n",
        "\n",
        "    Returns:\n",
        "        Output tensor after MLP computation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Project input: x -> gate, up\n",
        "    # x: [bs, seqlen, dim], w_gate: [dim, hidden_dim], w_up: [dim, hidden_dim]\n",
        "    gate = jnp.einsum(\"bsd,dh->bsh\", x, params.w_gate)\n",
        "    up = jnp.einsum(\"bsd,dh->bsh\", x, params.w_up)\n",
        "\n",
        "    # Apply the specified activation function (SwiGLU style)\n",
        "    if activation_fn == \"silu\":\n",
        "        activated_gate = jax.nn.silu(gate)\n",
        "    elif activation_fn == \"relu\":\n",
        "        activated_gate = jax.nn.relu(gate)\n",
        "    elif activation_fn == \"gelu\":\n",
        "        # Use approximate=False for exact GELU, True for faster approximation\n",
        "        activated_gate = jax.nn.gelu(gate, approximate=False)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
        "\n",
        "    fused_activation = activated_gate * up\n",
        "\n",
        "    # Project down\n",
        "    # fused_swiglu: [bs, seqlen, hidden_dim], w_down: [hidden_dim, dim]\n",
        "    output = jnp.einsum(\"bsh,hd->bsd\", fused_activation, params.w_down)\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch FeedForward implementation\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        hidden_dim: int,\n",
        "        multiple_of: int,\n",
        "        ffn_dim_multiplier: Optional[float],\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # hidden dim gymnastics that Meta simplified only later\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        if ffn_dim_multiplier is not None:\n",
        "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup Parameters\n",
        "bsz = 4\n",
        "seqlen = 64\n",
        "dim = 1536\n",
        "multiple_of = 32\n",
        "dtype = np.float32\n",
        "\n",
        "# Mimic hidden_dim calculation from PyTorch implementation\n",
        "hidden_dim = (dim//3)*8\n",
        "\n",
        "print(f\"Batch size: {bsz}\")\n",
        "print(f\"Sequence length: {seqlen}\")\n",
        "print(f\"Model dimension: {dim}\")\n",
        "print(f\"Hidden dimension: {hidden_dim}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Create shared weights and inputs\n",
        "np.random.seed(0)\n",
        "x_np = np.random.randn(bsz, seqlen, dim).astype(dtype)\n",
        "\n",
        "# PyTorch names weights w1, w2, w3. JAX uses w_gate, w_up, w_down.\n",
        "# Mapping: torch.w1 -> jax.w_gate, torch.w3 -> jax.w_up, torch.w2 -> jax.w_down\n",
        "w1_np = np.random.normal(0, 0.02, (dim, hidden_dim)).astype(dtype)  # gate_proj\n",
        "w3_np = np.random.normal(0, 0.02, (dim, hidden_dim)).astype(dtype)  # up_proj\n",
        "w2_np = np.random.normal(0, 0.02, (hidden_dim, dim)).astype(dtype)  # down_proj\n",
        "\n",
        "print(f\"Input shape: {x_np.shape}\")\n",
        "print(f\"Weight shapes: w1={w1_np.shape}, w2={w2_np.shape}, w3={w3_np.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. JAX setup\n",
        "x_jax = jnp.array(x_np, dtype=jax_dtype)\n",
        "jax_params = FeedForwardParams(\n",
        "    w_gate=jnp.array(w1_np, dtype=jax_dtype), \n",
        "    w_up=jnp.array(w3_np, dtype=jax_dtype), \n",
        "    w_down=jnp.array(w2_np, dtype=jax_dtype)\n",
        ")\n",
        "output_jax = feed_forward(x_jax, jax_params, \"silu\")\n",
        "\n",
        "print(f\"JAX output shape: {output_jax.shape}\")\n",
        "print(f\"JAX output dtype: {output_jax.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. PyTorch setup\n",
        "x_torch = torch.tensor(x_np, device=device, dtype=torch_dtype)\n",
        "torch_ff = FeedForward(\n",
        "    dim=dim, hidden_dim=hidden_dim, multiple_of=multiple_of, ffn_dim_multiplier=None\n",
        ")\n",
        "torch_ff.w1.weight = nn.Parameter(torch.tensor(w1_np.T, device=device, dtype=torch_dtype))\n",
        "torch_ff.w3.weight = nn.Parameter(torch.tensor(w3_np.T, device=device, dtype=torch_dtype))\n",
        "torch_ff.w2.weight = nn.Parameter(torch.tensor(w2_np.T, device=device, dtype=torch_dtype))\n",
        "output_torch = torch_ff(x_torch)\n",
        "\n",
        "print(f\"PyTorch output shape: {output_torch.shape}\")\n",
        "print(f\"PyTorch output dtype: {output_torch.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Compare outputs\n",
        "output_jax_np = np.array(output_jax)\n",
        "output_torch_np = output_torch.float().detach().cpu().numpy()\n",
        "\n",
        "# Check shapes match\n",
        "assert output_jax_np.shape == output_torch_np.shape, f\"Shape mismatch: JAX {output_jax_np.shape} vs PyTorch {output_torch_np.shape}\"\n",
        "\n",
        "# Compare with same tolerances as test_ops.py\n",
        "np.testing.assert_allclose(\n",
        "    output_jax_np, output_torch_np, rtol=1e-2, atol=1e-3\n",
        ")\n",
        "\n",
        "print(\"âœ“ Feedforward test passed!\")\n",
        "print(f\"Max absolute difference: {np.max(np.abs(output_jax_np - output_torch_np))}\")\n",
        "print(f\"Mean absolute difference: {np.mean(np.abs(output_jax_np - output_torch_np))}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
