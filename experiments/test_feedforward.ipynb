{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from experiments.torch_llama import FeedForward as FeedForward_torch\n",
        "from utils.ops import (\n",
        "    FeedForwardParams,\n",
        "    feed_forward as feed_forward_jax,\n",
        ")\n",
        "\n",
        "# Try to import torch_xla for TPU support\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    TORCH_XLA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_XLA_AVAILABLE = False\n",
        "\n",
        "# Device selection: TPU > CUDA > CPU\n",
        "if TORCH_XLA_AVAILABLE:\n",
        "    device = torch_xla.device()  # TPU device\n",
        "    print(f\"Using PyTorch XLA device: {device}\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using CUDA device\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU device\")\n",
        "\n",
        "jax_dtype = jnp.bfloat16\n",
        "torch_dtype = torch.bfloat16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup Parameters\n",
        "bsz = 4\n",
        "seqlen = 64\n",
        "dim = 1536\n",
        "multiple_of = 32\n",
        "dtype = np.float32\n",
        "\n",
        "# Mimic hidden_dim calculation from PyTorch implementation\n",
        "hidden_dim = (dim//3)*8\n",
        "\n",
        "print(f\"Batch size: {bsz}\")\n",
        "print(f\"Sequence length: {seqlen}\")\n",
        "print(f\"Model dimension: {dim}\")\n",
        "print(f\"Hidden dimension: {hidden_dim}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Create shared weights and inputs\n",
        "np.random.seed(0)\n",
        "x_np = np.random.randn(bsz, seqlen, dim).astype(dtype)\n",
        "\n",
        "# PyTorch names weights w1, w2, w3. JAX uses w_gate, w_up, w_down.\n",
        "# Mapping: torch.w1 -> jax.w_gate, torch.w3 -> jax.w_up, torch.w2 -> jax.w_down\n",
        "w1_np = np.random.normal(0, 0.02, (dim, hidden_dim)).astype(dtype)  # gate_proj\n",
        "w3_np = np.random.normal(0, 0.02, (dim, hidden_dim)).astype(dtype)  # up_proj\n",
        "w2_np = np.random.normal(0, 0.02, (hidden_dim, dim)).astype(dtype)  # down_proj\n",
        "\n",
        "print(f\"Input shape: {x_np.shape}\")\n",
        "print(f\"Weight shapes: w1={w1_np.shape}, w2={w2_np.shape}, w3={w3_np.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. JAX setup\n",
        "x_jax = jnp.array(x_np, dtype=jax_dtype)\n",
        "jax_params = FeedForwardParams(\n",
        "    w_gate=jnp.array(w1_np, dtype=jax_dtype), \n",
        "    w_up=jnp.array(w3_np, dtype=jax_dtype), \n",
        "    w_down=jnp.array(w2_np, dtype=jax_dtype)\n",
        ")\n",
        "output_jax = feed_forward_jax(x_jax, jax_params, \"silu\")\n",
        "\n",
        "print(f\"JAX output shape: {output_jax.shape}\")\n",
        "print(f\"JAX output dtype: {output_jax.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. PyTorch setup\n",
        "x_torch = torch.tensor(x_np, device=device, dtype=torch_dtype)\n",
        "torch_ff = FeedForward_torch(\n",
        "    dim=dim, hidden_dim=hidden_dim, multiple_of=multiple_of, ffn_dim_multiplier=None\n",
        ")\n",
        "torch_ff.w1.weight = torch.nn.Parameter(torch.tensor(w1_np.T, device=device, dtype=torch_dtype))\n",
        "torch_ff.w3.weight = torch.nn.Parameter(torch.tensor(w3_np.T, device=device, dtype=torch_dtype))\n",
        "torch_ff.w2.weight = torch.nn.Parameter(torch.tensor(w2_np.T, device=device, dtype=torch_dtype))\n",
        "output_torch = torch_ff(x_torch)\n",
        "\n",
        "print(f\"PyTorch output shape: {output_torch.shape}\")\n",
        "print(f\"PyTorch output dtype: {output_torch.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Compare outputs\n",
        "output_jax_np = np.array(output_jax)\n",
        "output_torch_np = output_torch.float().detach().cpu().numpy()\n",
        "\n",
        "# Check shapes match\n",
        "assert output_jax_np.shape == output_torch_np.shape, f\"Shape mismatch: JAX {output_jax_np.shape} vs PyTorch {output_torch_np.shape}\"\n",
        "\n",
        "# Compare with same tolerances as test_ops.py\n",
        "np.testing.assert_allclose(\n",
        "    output_jax_np, output_torch_np, rtol=1e-2, atol=1e-3\n",
        ")\n",
        "\n",
        "print(\"âœ“ Feedforward test passed!\")\n",
        "print(f\"Max absolute difference: {np.max(np.abs(output_jax_np - output_torch_np))}\")\n",
        "print(f\"Mean absolute difference: {np.mean(np.abs(output_jax_np - output_torch_np))}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
