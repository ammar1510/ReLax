{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from models.llama.tokenizer import Tokenizer # Assumes tokenizer.py is in the same directory orPYTHONPATH\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = \"\" # Add this line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_filename = \"dummy_custom_model.tiktoken\"\n",
    "\n",
    "    # A BPE ranks file where tokens align with pat_str behavior.\n",
    "    # \"token\"   -> dG9rZW4=\n",
    "    # \"1\"       -> MQ==\n",
    "    # \"2\"       -> Mg==\n",
    "    # \" \"       -> IA==\n",
    "    # \"b\"       -> Yg==\n",
    "    # \"c\"       -> Yw==\n",
    "    # \"a\"       -> YQ== \n",
    "    # \"e\"       -> ZQ==\n",
    "    # \"o\"       -> bw==\n",
    "    # \"t\"       -> dA==\n",
    "    # \"i\"       -> aQ==\n",
    "    # \"d\"       -> ZA==\n",
    "    # \"eo\"      -> ZW8=\n",
    "    # \"ot\"      -> b3Q=\n",
    "    # \"eot\"     -> ZW90\n",
    "    # \"<\"       -> PA==\n",
    "    # \"|\"       -> fA==\n",
    "    # \"_\"       -> Xw==\n",
    "    # \"id\"      -> aWQ=\n",
    "    # \">\"       -> Pg==\n",
    "dummy_model_content= \"\"\"\n",
    "dG9rZW4= 0\n",
    "MQ== 1\n",
    "Mg== 2\n",
    "IA== 3\n",
    "Yg== 4\n",
    "Yw== 5\n",
    "YQ== 6\n",
    "ZQ== 7\n",
    "bw== 8\n",
    "dA== 9\n",
    "aQ== 10\n",
    "ZA== 11\n",
    "ZW8= 12\n",
    "b3Q= 13\n",
    "ZW90 14\n",
    "PA== 15\n",
    "fA== 16\n",
    "Xw== 17\n",
    "aWQ= 18\n",
    "Pg== 19\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy tokenizer model: dummy_custom_model.tiktoken\n"
     ]
    }
   ],
   "source": [
    "processed_content = dummy_model_content.strip()\n",
    "\n",
    "with open(dummy_model_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(processed_content)\n",
    "print(f\"Created dummy tokenizer model: {dummy_model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing tokenizer with dummy model...\n",
      "{b'token': 0, b'1': 1, b'2': 2, b' ': 3, b'b': 4, b'c': 5, b'a': 6, b'e': 7, b'o': 8, b't': 9, b'i': 10, b'd': 11, b'eo': 12, b'ot': 13, b'eot': 14, b'<': 15, b'|': 16, b'_': 17, b'id': 16, b'>': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at src/lib.rs:482:9:\n",
      "Encoder and decoder must be of equal length; maybe you had duplicate token indices in your encoder?\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "Encoder and decoder must be of equal length; maybe you had duplicate token indices in your encoder?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPanicException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. Initialize the Tokenizer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Note: The Tokenizer class defines its own special tokens like <|eot_id|>,\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# which will be added on top of the base vocabulary from the dummy file.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInitializing tokenizer with dummy model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer = \u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_model_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTokenizer initialized successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/models/llama/tokenizer.py:68\u001b[39m, in \u001b[36mTokenizer.__init__\u001b[39m\u001b[34m(self, model_path)\u001b[39m\n\u001b[32m     49\u001b[39m special_tokens = [\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<|begin_of_text|>\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<|end_of_text|>\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_reserved_special_tokens - \u001b[32m5\u001b[39m)\n\u001b[32m     63\u001b[39m ]\n\u001b[32m     64\u001b[39m \u001b[38;5;28mself\u001b[39m.special_tokens = {\n\u001b[32m     65\u001b[39m     token: num_base_tokens + i \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(special_tokens)\n\u001b[32m     66\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mtiktoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEncoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpat_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpat_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmergeable_ranks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmergeable_ranks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_size: \u001b[38;5;28mint\u001b[39m = \u001b[38;5;28mself\u001b[39m.model.n_vocab\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# BOS / EOS token IDs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReLax/.venv/lib/python3.12/site-packages/tiktoken/core.py:54\u001b[39m, in \u001b[36mEncoding.__init__\u001b[39m\u001b[34m(self, name, pat_str, mergeable_ranks, special_tokens, explicit_n_vocab)\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mergeable_ranks) + \u001b[38;5;28mlen\u001b[39m(special_tokens) == explicit_n_vocab\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_token_value == explicit_n_vocab - \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28mself\u001b[39m._core_bpe = \u001b[43m_tiktoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCoreBPE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmergeable_ranks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpat_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPanicException\u001b[39m: Encoder and decoder must be of equal length; maybe you had duplicate token indices in your encoder?"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Initialize the Tokenizer\n",
    "# Note: The Tokenizer class defines its own special tokens like <|eot_id|>,\n",
    "# which will be added on top of the base vocabulary from the dummy file.\n",
    "print(\"\\nInitializing tokenizer with dummy model...\")\n",
    "tokenizer = Tokenizer(model_path=dummy_model_filename)\n",
    "print(\"Tokenizer initialized successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Invalid base64-encoded string: number of data characters (1) cannot be 1 more than a multiple of 4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbase64\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbase64\u001b[49m\u001b[43m.\u001b[49m\u001b[43mb64decode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/base64.py:88\u001b[39m, in \u001b[36mb64decode\u001b[39m\u001b[34m(s, altchars, validate)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(altchars) == \u001b[32m2\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(altchars)\n\u001b[32m     87\u001b[39m     s = s.translate(\u001b[38;5;28mbytes\u001b[39m.maketrans(altchars, \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m+/\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinascii\u001b[49m\u001b[43m.\u001b[49m\u001b[43ma2b_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mError\u001b[39m: Invalid base64-encoded string: number of data characters (1) cannot be 1 more than a multiple of 4"
     ]
    }
   ],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ZA=='\n"
     ]
    }
   ],
   "source": [
    "print(base64.b64encode(b\"d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
